{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2e566f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/261c3e30bae8b8bdc83541ffa9331b52fcf015e6/3-Figure2-1.png\" width=50% >\n",
    "\n",
    "# <center> Assignment 1: Photometric Stereo & Colour </center>\n",
    "<center> Computer Vision 1 University of Amsterdam </center>\n",
    "    <center> Due 23:59 PM, September 17, 2022 (Amsterdam time) </center>\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f383ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## General guidelines\n",
    "Your code and discussion must be submitted through this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 1 Assignment. For full credit, make sure your answer follows these guidelines:\n",
    "- Please express your thoughts concisely. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make your notebook self-contained and complete.\n",
    "- Analyze your results and discuss them, e.g. why algorithm A works better than algorithm B in a certain problem.\n",
    "- Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0 Environment Set Up\n",
    "\n",
    "You are allowed to use **only** the following library versions:\n",
    "- python=3.10.4\n",
    "- matplotlib==3.5.3\n",
    "- matplotlib-inline==0.1.6\n",
    "- numpy==1.23.2\n",
    "- opencv-python==4.6.0.66\n",
    "\n",
    "Using functions that are not working in these versions could lead to grade deduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa2dc5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Photometric Stereo (50pts)\n",
    "\n",
    "In this part of the assignment, you are going to implement the photometric stereo algorithm as described in Section 5.4 (Forsyth and Ponce, *Computer Vision: A Modern Approach*). The chapter snippet can be found in the course materials.\n",
    "\n",
    "Following this instruction, you will have to edit and fill in your code in the functions **estimate_alb_nrm**, **check_integrability**, and **construct_surface**. The main function **photometric_stereo** is provided for reference and should not be taken as is. Throughout the assignment, you will be asked to perform different trials and experiments which will require you to adjust the main code accordingly, this also shows how well you can cope with the materials.\n",
    "\n",
    "Include images of the results into this notebook. For 3D models, make sure to choose a viewpoint that makes the structure as clear as possible and/or feel free to take them from multiple viewpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caca639",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Estimating Albedo and Surface Normal (15pts)\n",
    "Let us start with the grayscale sphere model, which is located in the SphereGray5 folder. The folder contains 5 images of a sphere with grayscale checker texture under similar lighting conditions with the one in the book. Your task is to estimate the surface reflectance (albedo) and surface normal of this model. The light source directions are encoded in the image file names.\n",
    "\n",
    "1. Complete the code for function **estimate_alb_nrm()** to estimate albedo and surface normal map for the SphereGray5 folder. What do you expect to see in albedo image and how is it different with your result?\n",
    "2. In principle, what is the minimum number of images you need to estimate albedo and surface normal? Run the algorithm with more images by using SphereGray25 and observe the differences in the results. You could try all images at once or a few at the time, in an incremental fashion. Choose a strategy and justify it by discussing your results.\n",
    "3. What is the impact of shadows in photometric stereo? Explain the trick that is used in the text to deal with shadows. Remove that trick and check your results. Is the trick necessary in the case of 5 images, how about 25 images?\n",
    "\n",
    "**Hint**: To get the least-squares solution of a linear system, you can use **numpy.linalg.lstsq** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0c3b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First some utils we need\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def load_syn_images(image_dir='./SphereGray5/', channel=0):\n",
    "    files = os.listdir(image_dir)\n",
    "    #files = [os.path.join(image_dir, f) for f in files]\n",
    "    nfiles = len(files)\n",
    "    \n",
    "    image_stack = None\n",
    "    V = 0\n",
    "    Z = 0.5\n",
    "    \n",
    "    for i in range(nfiles):\n",
    "        # read input image\n",
    "        im = cv2.imread(os.path.join(image_dir, files[i]))\n",
    "        im = im[:,:,channel]\n",
    "        \n",
    "        # stack at third dimension\n",
    "        if image_stack is None:\n",
    "            h, w = im.shape\n",
    "            print('Image size (H*W): %d*%d' %(h,w) )\n",
    "            image_stack = np.zeros([h, w, nfiles], dtype=int)\n",
    "            V = np.zeros([nfiles, 3], dtype=np.float64)\n",
    "            \n",
    "        image_stack[:,:,i] = im\n",
    "        \n",
    "        # read light direction from image name\n",
    "        X = np.double(files[i][(files[i].find('_')+1):files[i].rfind('_')])\n",
    "        Y = np.double(files[i][files[i].rfind('_')+1:files[i].rfind('.png')])\n",
    "        V[i, :] = [-X, Y, Z]\n",
    "        \n",
    "    # normalization\n",
    "    image_stack = np.double(image_stack)\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    normV = np.tile(np.sqrt(np.sum(V ** 2, axis=1, keepdims=True)), (1, V.shape[1]))\n",
    "    scriptV = V / normV\n",
    "    \n",
    "    return image_stack, scriptV\n",
    "    \n",
    "    \n",
    "def load_face_images(image_dir='./yaleB02/'):\n",
    "    num_images = 64\n",
    "    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n",
    "    ambient_image = cv2.imread(filename, -1)\n",
    "    h, w = ambient_image.shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob \n",
    "    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    for j in range(num_images):\n",
    "        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n",
    "\n",
    "\n",
    "    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n",
    "    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n",
    "    z = np.sin(np.pi*ang[1,:]/180)\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def load_apple_images(image_dir='./Apples_png/'):\n",
    "    num_images = 100\n",
    "    filename = os.path.join(image_dir, 'I_0000.png')\n",
    "    try_image = cv2.imread(filename, -1)\n",
    "    h, w = try_image[:,:,0].shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob \n",
    "    d = glob.glob(os.path.join(image_dir, 'I_00*.png'))\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "    filenames_idx = []\n",
    "    for i in filenames:\n",
    "        filenames_idx.append(int(i.split('_')[1].split('.')[0]))\n",
    "\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    for j in range(num_images):\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,0]\n",
    "        \n",
    "    with open('./images/photometrics_images/Apple/light_directions_refined.txt') as file:\n",
    "        lines = [line.split() for line in file]\n",
    "        x, y, z = [], [], []\n",
    "        for idx in filenames_idx:\n",
    "            x.append(float(lines[idx][0]))\n",
    "            y.append(float(lines[idx][1]))\n",
    "            z.append(float(lines[idx][2]))\n",
    "\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return image_stack, scriptV\n",
    "    \n",
    "    \n",
    "def show_results(albedo, normals, height_map, SE):\n",
    "    # Stride in the plot, you may want to adjust it to different images\n",
    "    stride = 1\n",
    "    \n",
    "    # showing albedo map\n",
    "    fig = plt.figure()\n",
    "    albedo_max = albedo.max()\n",
    "    albedo_max = 1\n",
    "    albedo = albedo / albedo_max\n",
    "    print(albedo.shape)\n",
    "    plt.imshow(albedo, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure()\n",
    "    ax1 = figure.add_subplot(131)\n",
    "    ax1.imshow(normals[..., 0])\n",
    "    ax2 = figure.add_subplot(132)\n",
    "    ax2.imshow(normals[..., 1])\n",
    "    ax3 = figure.add_subplot(133)\n",
    "    ax3.imshow(normals[..., 2])\n",
    "    plt.show()\n",
    "    \n",
    "    # meshgrid\n",
    "    X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], stride),\n",
    "    np.arange(0,np.shape(normals)[1], stride),\n",
    "    np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "    \n",
    "    '''\n",
    "    =============\n",
    "    You could further inspect the shape of the objects and normal directions by using plt.quiver() function.  \n",
    "    =============\n",
    "    '''\n",
    "    \n",
    "    # plotting the SE\n",
    "    H = SE[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.show()\n",
    "    \n",
    "    # plotting model geometry\n",
    "    H = height_map[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27cb30",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_alb_nrm( image_stack, scriptV, shadow_trick=True):\n",
    "    \n",
    "    # COMPUTE_SURFACE_GRADIENT compute the gradient of the surface\n",
    "    # INPUT:\n",
    "    # image_stack : the images of the desired surface stacked up on the 3rd dimension\n",
    "    # scriptV : matrix V (in the algorithm) of source and camera information\n",
    "    # shadow_trick: (true/false) whether or not to use shadow trick in solving linear equations\n",
    "    # OUTPUT:\n",
    "    # albedo : the surface albedo\n",
    "    # normal : the surface normal\n",
    "\n",
    "    h, w, _ = image_stack.shape\n",
    "    \n",
    "    # create arrays for \n",
    "    # albedo (1 channel)\n",
    "    # normal (3 channels)\n",
    "    albedo = np.zeros([h, w])\n",
    "    normal = np.zeros([h, w, 3])\n",
    "    \n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    for each point in the image array\n",
    "        stack image values into a vector i\n",
    "        construct the diagonal matrix scriptI\n",
    "        solve scriptI * scriptV * g = scriptI * i to obtain g for this point\n",
    "        albedo at this point is |g|\n",
    "        normal at this point is g / |g|\n",
    "    \"\"\"\n",
    "    \n",
    "    return albedo, normal\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     n = 5\n",
    "#     image_stack = np.zeros([10,10,n])\n",
    "#     scriptV = np.zeros([n,3])\n",
    "#     estimate_alb_nrm( image_stack, scriptV, shadow_trick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0425a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Test of Integrability (10pts)\n",
    "\n",
    "Before we can reconstruct the surface height map, it is required to compute the partial derivatives $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$ (or *p* and *q* in the algorithm). The partial derivatives also give us a chance to double check our computation, namely the test of *integrability*.\n",
    "\n",
    "1. Compute the partial derivatives (p and q in the algorithm) by filling in your code into **check_integrability()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652c1fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_integrability(normals):\n",
    "    #  CHECK_INTEGRABILITY check the surface gradient is acceptable\n",
    "    #   normals: normal image\n",
    "    #   p : df / dx\n",
    "    #   q : df / dy\n",
    "    #   SE : Squared Errors of the 2 second derivatives\n",
    "\n",
    "    # initalization\n",
    "    p = np.zeros(normals.shape[:2])\n",
    "    q = np.zeros(normals.shape[:2])\n",
    "    SE = np.zeros(normals.shape[:2])\n",
    "    \n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    Compute p and q, where\n",
    "    p measures value of df / dx\n",
    "    q measures value of df / dy\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # change nan to 0\n",
    "    p[p!=p] = 0\n",
    "    q[q!=q] = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    approximate second derivate by neighbor difference\n",
    "    and compute the Squared Errors SE of the 2 second derivatives SE\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return p, q, SE\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     normals = np.zeros([10,10,3])\n",
    "#     check_integrability(normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f6d09",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Implement and compute the second derivatives according to the algorithm and perform the test of integrability by choosing a reasonable threshold. What could be the reasons for the errors? How does the test perform with different number of images used in the reconstruction process in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb87a7e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a42de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Shape by Integration (10pts)\n",
    "To reconstruct the surface height map, we need to continuously integrate the partial derivatives over a path. However, as we are working with discrete structures, you will be simply summing their values.\n",
    "\n",
    "The algorithm in the chapter presents a way to do the integration in column-major order, that is you start at the top-left corner and integrate along the first column, then go towards right along each row. Yet, it is also noticed that it would be better to use many different paths and average so as to spread around the errors in the derivative estimates.\n",
    "\n",
    "1. Construct the surface height map using column-major order as described in the algorithm, then implement row-major path integration. Your code should now go to **construct_surface()**.\n",
    "\n",
    "**Note**: By default, Numpy used row-major operations. So if you are unrolling an image to linearize the operation, you will end up with a row-major representation. Numpy can be configured to be column-major. Otherwise, if you are using the double for-loops without an unrolling operation, then this concern doesn’t apply.\n",
    "\n",
    "**Hint**: You could further inspect the shape of the objects and normal directions by using **matplotlib.pyplot.quiver** function. You will have to choose appropriate sub-sampling ratios for proper illustration. You code goes to the **show_results()** in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaf5bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def construct_surface(p, q, path_type='column'):\n",
    "\n",
    "    '''\n",
    "    CONSTRUCT_SURFACE construct the surface function represented as height_map\n",
    "       p : measures value of df / dx\n",
    "       q : measures value of df / dy\n",
    "       path_type: type of path to construct height_map, either 'column',\n",
    "       'row', or 'average'\n",
    "       height_map: the reconstructed surface\n",
    "    '''\n",
    "    \n",
    "    h, w = p.shape\n",
    "    height_map = np.zeros([h, w])\n",
    "    \n",
    "    if path_type=='column':\n",
    "        \"\"\"\n",
    "        ================\n",
    "        Your code here\n",
    "        ================\n",
    "        % top left corner of height_map is zero\n",
    "        % for each pixel in the left column of height_map\n",
    "        %   height_value = previous_height_value + corresponding_q_value\n",
    "        \n",
    "        % for each row\n",
    "        %   for each element of the row except for leftmost\n",
    "        %       height_value = previous_height_value + corresponding_p_value\n",
    "        \n",
    "        \"\"\"\n",
    "    elif path_type=='row':\n",
    "        \"\"\"\n",
    "        ================\n",
    "        Your code here\n",
    "        ================\n",
    "        \"\"\"\n",
    "    elif path_type=='average':\n",
    "        \"\"\"\n",
    "        ================\n",
    "        Your code here\n",
    "        ================\n",
    "        \"\"\"\n",
    "        \n",
    "    return height_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389c62a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. What are the differences in the results of the two paths?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d1a05",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc400508",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Now, take the average of the results. Do you see any improvement compared to when using only one path? Are the construction results different with different number of images being used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea353ec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a92cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Experiments with different objects (15pts)\n",
    "In this part, you will try to run the photometric stereo algorithm in various number of scenarios to see how well it can be generalized.\n",
    "\n",
    "1. Run the algorithm and show the results for the MonkeyGray model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a81909",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo(image_dir='./SphereGray5/' ):\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_syn_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface gradient from the stack of imgs and light source mat\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    [albedo, normals] = estimate_alb_nrm(image_stack, scriptV)\n",
    "\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "    [p, q, SE] = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface( p, q )\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "\n",
    "## Face\n",
    "def photometric_stereo_face(image_dir='./yaleB02/'):\n",
    "    [image_stack, scriptV] = load_face_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    albedo, normals = estimate_alb_nrm(image_stack, scriptV)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking')\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface(p, q )\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "    \n",
    "## Apple\n",
    "def photometric_stereo_apple(image_dir='./Apple_png/'):\n",
    "    [image_stack, scriptV] = load_apple_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    albedo, normals = estimate_alb_nrm(image_stack, scriptV)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking')\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface(p, q )\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     photometric_stereo()\n",
    "#     photometric_stereo_face()\n",
    "#     photometric_stereo_apple()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c44adc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. The albedo results of the monkey may comprise more albedo errors than in case of the sphere. Observe and describe the errors. What could be the reason for those errors? You may want to experiment with different number of images as you did in Question 1 to see the effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300d077",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9e17d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. How do you think that could help solving these errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378f616",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d05c2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So far, we have assumed that albedos are 1-channel grayscale images and that input images are also 1-channel. To work with 3-channel images, a simple solution is to split the input image into separate channels and treat them individually. Yet, that would generate a small problem while constructing the surface normal map if a pixel value in a channel is zero.\n",
    "\n",
    "1. Update the implementation to work for 3-channel RGB inputs and test it with 2 models SphereColor and MonkeyColor. \n",
    "2. Explain your changes and show your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da16d95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388254ec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Observe the problem in the constructed surface normal map and height map, explain why a zero pixel could be a problem and propose a way to overcome that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405a82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d6338",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it's the time to try the algorithm on real-world datasets. For that purpose, we are going to use the [Yale Face Database](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html)\n",
    "\n",
    "1. Run the algorithm for the Yale Face images (included in the lab material). \n",
    "2. Observe and discuss the results for different integration paths. \n",
    "\n",
    "**Hint**: For proper computation of albedo and surface normal, you may want to suspend the shadow trick described in the text, and use the original formula:\n",
    "$$i = Vg(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df034f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4f6ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Discuss how the images violate the assumptions of the shape-from-shading methods. Remember to include specific input images to illustrate your points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e71de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc40200",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. How would the results improve when the problematic images are all removed? Try it out and show the results here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e1c89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dde1f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, show your results on real-world 3-channel RGB inputs, contained in the \"Apple\" folder, taken from [this dataset](http://vision.ucsd.edu/~nalldrin/research/cvpr08/datasets/) from the University of California San Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeaf07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Observe and discuss the results for different integration paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5896dbe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Colour Spaces (20pts)\n",
    "\n",
    "In this part of the assignment, you will study the different colour spaces for image representations and experiment how to convert a given RGB image to a specific colour space.\n",
    "\n",
    "### 2.1 RGB Colour Model (3pts)\n",
    "\n",
    "Why do we use RGB colour model as a basis of our digital cameras and photography? How does a standard digital camera capture the full RGB colour image?\n",
    "\n",
    "### 2.2 Colour Space Conversion (10pts)\n",
    "\n",
    "Create a function to convert an RGB image into the following colour spaces by using the template code you are provided ConvertColourSpace.py and other sub-functions. Visualize the new image and its channels separately in the same figure. That is, for example, in the case of HSV colour space, you need to visualize the converted HSV image, and its Hue, Saturation and Value channels separately (4 images, 1 figure). Do not change the already given code.\n",
    "\n",
    "__Opponent Colour Space__\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "O_1 \\\\\n",
    "O_2 \\\\\n",
    "O_3 \n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "\\frac{R-G}{\\sqrt{2}} \\\\\n",
    "\\frac{R+G-2B}{\\sqrt{6}} \\\\\n",
    "\\frac{R+G+B}{\\sqrt{3}} \n",
    "\\end{pmatrix}$ \n",
    "\n",
    "__Normalized RGB (rgb) Colour Space__\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "r \\\\\n",
    "g \\\\\n",
    "b \n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "\\frac{R}{R+G+B} \\\\\n",
    "\\frac{G}{R+G+B} \\\\\n",
    "\\frac{B}{R+G+B} \n",
    "\\end{pmatrix}$ \n",
    "\n",
    "__HSV Colour Space__\n",
    "\n",
    "Convert the RGB image into HSV Colour Space. Use OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2HSV)*.\n",
    "\n",
    "__YCbCr Colour Space__\n",
    "\n",
    "Convert the RGB image into YCbCr Colour Space. Use OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2YCrCb)*. Note, you need to arrange the channels in $Y, C_b$ and $C_r$ order.\n",
    "\n",
    "__Grayscale__\n",
    "\n",
    "Convert the RGB image into grayscale by using 3 different methods mentioned in\n",
    "https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/\n",
    "In the end, check and report which method OpenCV uses for grayscale conversion, include it as well, and visualize all 4 in the same figure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378a05e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283cd2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Colour Space Properties (5pts)\n",
    "\n",
    "Explain each of those 5 colour spaces and their properties. What are the benefits of using a different colour space other than RGB? Provide reasons for each of the above cases. You can include your observations from the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae5311",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb04f78",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 More on Colour Spaces (2pts)\n",
    "\n",
    "Find one more colour space from the literature and simply explain its properties and give a use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b842e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b280abd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Intrinsic Image Decomposition (15pts)\n",
    "Intrinsic image decomposition is the process of separating an image into its formation components, such as reflectance (albedo) and shading (illumination). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Then, under the assumption of body (diffuse) reflection, linear sensor response and narrow band filters, the decomposition of the observed image $I(\\vec{x})$ at position $\\vec{x}$ can be approximated as the element-wise product of its albedo $R(\\vec{x})$ and shading $S(\\vec{x})$ intrinsics:\n",
    "\n",
    "$$I(\\vec{x})=R(\\vec{x}) \\times S(\\vec{x})$$\n",
    "\n",
    "In this part of the assignment, you will experiment with intrinsic image components to perform one of the computational photography applications; material recolouring. For the experiments, we will use images from a synthetic intrinsic image dataset. <a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2)\n",
    "\n",
    "<a name=\"cite_note-1\"></a><small>1. [^](#cite_ref-1) H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3-26, 1978.</small>\n",
    "\n",
    "<a name=\"cite_note-2\"></a><small>2. [^](#cite_ref-1) http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ef75c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. What other components can an image be decomposed other than albedo and shading Give an example and explain your reasoning. *(4pts)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e148a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d17f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. If you check the literature, you will see that almost all the intrinsic image decomposition datasets are composed of synthetic images. What might be the reason for that? *(2pts)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52379ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a267f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Show that you can actually reconstruct the original *turtle.png* image from its intrinsics using *turtle_albedo.png* and *turtle_shading.png*. In the end, your script should output a figure displaying the original image, its intrinsic images and the reconstructed one. Complete the code for function **iid_image_formation()**. *(4pts)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07d3c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def iid_image_formation(albedo_img, shading_img):\n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc26735",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Recoloring *(5pts)*\n",
    "Manipulating colours in photographs is an important problem with many applications in computer vision. Since the aim for recolouring algorithms is just to manipulate colours, better results can be obtained for such a task if the albedo image is available as it is independent of confounding illumination effects.\n",
    "\n",
    "Assume that you are given the *turtle.png* image and you have access to its\n",
    "intrinsic images *turtle_albedo.png* and *turtle_shading.png*.\n",
    "1. Find out the true material colour of the turtle in RGB space (which is uniform in this case).\n",
    "2. Recolour the turtle image with pure green (0, 255, 0). Display the original turtle image and the recoloured version on the same figure. Complete the code for function **recoloring()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15435d05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def recoloring(albedo_img, shading_img):\n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d94cb2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Although you have recoloured the object with pure green, the reconstructed images do not seem to display those pure colors and thus the colour distributions over the object do not appear uniform. Explain the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a44258",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6030a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that this was a simple case where the image is synthetic, object centered and has only one colour, and you have access to its ground-truth intrinsic images. Real world scenarios require more than just replacing a single colour with another, not to mention the complexity of achieving a decent intrinsic image decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb41c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 Colour Constancy (15pts)\n",
    "\n",
    "Colour constancy is the ability to perceive colors of objects, invariant to the colour of the light source. The aim for colour constancy algorithms is first to estimate the illuminant of the light source, and then correct the image so that the corrected image appears to be taken under a canonical (white) light source. The task of the automatic white balance (AWB) is to do the same in digital cameras so that the images taken by a digital camera look as natural as possible.\n",
    "\n",
    "In this part of the assignment, you will implement the most famous colour constancy algorithm; *Grey-World Algorithm*.\n",
    "\n",
    "### Grey-World Algorithm\n",
    "The algorithm assumes that, under a white light source, the average colour in a scene should be achromatic (grey, [128, 128, 128]).\n",
    "\n",
    "1. Complete the function to apply colour correction to an RGB image by using Grey-World algorithm. Display the original image and the colour corrected one on the same figure. Use awb.jpg image to test your algorithm. In the end, you should see that the reddish colour cast on the image is removed and it looks more natural.\n",
    "\n",
    "  ***Note:*** You do not need to apply any pre or post processing steps. For the calculation or processing, you are not allowed to use any available code or any dedicated library function except *standard Numpy functions*.\n",
    "   \n",
    "   ***Hint:*** Check the von Kries model for this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb3984",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def grey_world(awb_img):\n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c12a2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Give an example case for Grey-World Algorithm on where it might fail. Remember to include your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0aaaf7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2715e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Find out one more colour constancy algorithms from the literature and explain it briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8785c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2fc6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}