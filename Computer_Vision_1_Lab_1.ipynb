{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2e566f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/261c3e30bae8b8bdc83541ffa9331b52fcf015e6/3-Figure2-1.png\" width=50% >\n",
    "\n",
    "# <center> Assignment 1: Photometric Stereo & Colour </center>\n",
    "<center> Computer Vision 1 University of Amsterdam </center>\n",
    "    <center> Due 23:59 PM, September 17, 2022 (Amsterdam time) </center>\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f383ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## General guidelines\n",
    "Your code and discussion must be submitted through this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 1 Assignment. For full credit, make sure your answer follows these guidelines:\n",
    "- Please express your thoughts concisely. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make your notebook self-contained and complete.\n",
    "- Analyze your results and discuss them, e.g. why algorithm A works better than algorithm B in a certain problem.\n",
    "- Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAsâ€™ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0 Environment Set Up\n",
    "\n",
    "You are allowed to use **only** the following library versions:\n",
    "- python=3.10.4\n",
    "- matplotlib==3.5.3\n",
    "- matplotlib-inline==0.1.6\n",
    "- numpy==1.23.2\n",
    "- opencv-python==4.6.0.66\n",
    "\n",
    "Using functions that are not working in these versions could lead to grade deduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa2dc5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Photometric Stereo (50pts)\n",
    "\n",
    "In this part of the assignment, you are going to implement the photometric stereo algorithm as described in Section 5.4 (Forsyth and Ponce, *Computer Vision: A Modern Approach*). The chapter snippet can be found in the course materials.\n",
    "\n",
    "Following this instruction, you will have to edit and fill in your code in the functions **estimate_alb_nrm**, **check_integrability**, and **construct_surface**. The main function **photometric_stereo** is provided for reference and should not be taken as is. Throughout the assignment, you will be asked to perform different trials and experiments which will require you to adjust the main code accordingly, this also shows how well you can cope with the materials.\n",
    "\n",
    "Include images of the results into this notebook. For 3D models, make sure to choose a viewpoint that makes the structure as clear as possible and/or feel free to take them from multiple viewpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caca639",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Estimating Albedo and Surface Normal (15pts)\n",
    "Let us start with the grayscale sphere model, which is located in the SphereGray5 folder. The folder contains 5 images of a sphere with grayscale checker texture under similar lighting conditions with the one in the book. Your task is to estimate the surface reflectance (albedo) and surface normal of this model. The light source directions are encoded in the image file names.\n",
    "\n",
    "1. Complete the code for function **estimate_alb_nrm()** to estimate albedo and surface normal map for the SphereGray5 folder. What do you expect to see in albedo image and how is it different with your result?\n",
    "2. In principle, what is the minimum number of images you need to estimate albedo and surface normal? Run the algorithm with more images by using SphereGray25 and observe the differences in the results. You could try all images at once or a few at the time, in an incremental fashion. Choose a strategy and justify it by discussing your results.\n",
    "3. What is the impact of shadows in photometric stereo? Explain the trick that is used in the text to deal with shadows. Remove that trick and check your results. Is the trick necessary in the case of 5 images, how about 25 images?\n",
    "\n",
    "**Hint**: To get the least-squares solution of a linear system, you can use **numpy.linalg.lstsq** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880208f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First some utils we need\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def load_syn_images(image_dir='./SphereGray5/', channel=0):\n",
    "    files = os.listdir(image_dir)\n",
    "    #files = [os.path.join(image_dir, f) for f in files]\n",
    "    nfiles = len(files)\n",
    "\n",
    "    image_stack = None\n",
    "    V = 0\n",
    "    Z = 0.5\n",
    "\n",
    "    for i in range(nfiles):\n",
    "        # read input image\n",
    "        im = cv2.imread(os.path.join(image_dir, files[i]))\n",
    "        im = im[:,:,channel]\n",
    "\n",
    "        # stack at third dimension\n",
    "        if image_stack is None:\n",
    "            h, w = im.shape\n",
    "            print('Image size (H*W): %d*%d' %(h,w) )\n",
    "            image_stack = np.zeros([h, w, nfiles], dtype=int)\n",
    "            V = np.zeros([nfiles, 3], dtype=np.float64)\n",
    "\n",
    "        image_stack[:,:,i] = im\n",
    "\n",
    "        # read light direction from image name\n",
    "        X = np.double(files[i][(files[i].find('_')+1):files[i].rfind('_')])\n",
    "        Y = np.double(files[i][files[i].rfind('_')+1:files[i].rfind('.png')])\n",
    "        V[i, :] = [-X, Y, Z]\n",
    "\n",
    "    # normalization\n",
    "    image_stack = np.double(image_stack)\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    normV = np.tile(np.sqrt(np.sum(V ** 2, axis=1, keepdims=True)), (1, V.shape[1]))\n",
    "    scriptV = V / normV\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def load_face_images(image_dir='./yaleB02/'):\n",
    "    num_images = 64\n",
    "    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n",
    "    ambient_image = cv2.imread(filename, -1)\n",
    "    h, w = ambient_image.shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob\n",
    "    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    for j in range(num_images):\n",
    "        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n",
    "\n",
    "\n",
    "    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n",
    "    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n",
    "    z = np.sin(np.pi*ang[1,:]/180)\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def load_apple_images(image_dir='./Apples_png/'):\n",
    "    num_images = 100\n",
    "    filename = os.path.join(image_dir, 'I_0000.png')\n",
    "    try_image = cv2.imread(filename, -1)\n",
    "    h, w = try_image[:,:,0].shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob\n",
    "    d = glob.glob(os.path.join(image_dir, 'I_00*.png'))\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "    filenames_idx = []\n",
    "    for i in filenames:\n",
    "        filenames_idx.append(int(i.split('_')[1].split('.')[0]))\n",
    "\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    for j in range(num_images):\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,0]\n",
    "\n",
    "    with open('./images/photometrics_images/Apple/light_directions_refined.txt') as file:\n",
    "        lines = [line.split() for line in file]\n",
    "        x, y, z = [], [], []\n",
    "        for idx in filenames_idx:\n",
    "            x.append(float(lines[idx][0]))\n",
    "            y.append(float(lines[idx][1]))\n",
    "            z.append(float(lines[idx][2]))\n",
    "\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def show_results(albedo, normals, height_map, SE):\n",
    "    # Stride in the plot, you may want to adjust it to different images\n",
    "    stride = 1\n",
    "\n",
    "    # showing albedo map\n",
    "    fig = plt.figure()\n",
    "    albedo_max = albedo.max()\n",
    "    albedo_max = 1\n",
    "    albedo = albedo / albedo_max\n",
    "    print(albedo.shape)\n",
    "    plt.imshow(albedo, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure()\n",
    "    ax1 = figure.add_subplot(131)\n",
    "    ax1.imshow(normals[..., 0])\n",
    "    ax2 = figure.add_subplot(132)\n",
    "    ax2.imshow(normals[..., 1])\n",
    "    ax3 = figure.add_subplot(133)\n",
    "    ax3.imshow(normals[..., 2])\n",
    "    plt.show()\n",
    "\n",
    "    # meshgrid\n",
    "    X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], stride),\n",
    "    np.arange(0,np.shape(normals)[1], stride),\n",
    "    np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "\n",
    "    '''\n",
    "    =============\n",
    "    You could further inspect the shape of the objects and normal directions by using plt.quiver() function.\n",
    "    =============\n",
    "    '''\n",
    "\n",
    "    # plotting the SE\n",
    "    H = SE[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.show()\n",
    "\n",
    "    # plotting model geometry\n",
    "    H = height_map[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.title('geometry')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1c238",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_alb_nrm( image_stack, scriptV, shadow_trick=True):\n",
    "\n",
    "    # COMPUTE_SURFACE_GRADIENT compute the gradient of the surface\n",
    "    # INPUT:\n",
    "    # image_stack : the images of the desired surface stacked up on the 3rd dimension\n",
    "    # scriptV : matrix V (in the algorithm) of source and camera information\n",
    "    # shadow_trick: (true/false) whether or not to use shadow trick in solving linear equations\n",
    "    # OUTPUT:\n",
    "    # albedo : the surface albedo\n",
    "    # normal : the surface normal\n",
    "\n",
    "    h, w, _ = image_stack.shape\n",
    "\n",
    "    # create arrays for\n",
    "    # albedo (1 channel)\n",
    "    # normal (3 channels)\n",
    "    albedo = np.zeros([h, w])\n",
    "    normal = np.zeros([h, w, 3])\n",
    "\n",
    "    \"\"\"\n",
    "    for each point in the image array\n",
    "        stack image values into a vector i\n",
    "        construct the diagonal matrix scriptI\n",
    "        solve scriptI * scriptV * g = scriptI * i to obtain g for this point\n",
    "        albedo at this point is |g|\n",
    "        normal at this point is g / |g|\n",
    "    \"\"\"\n",
    "    for x in range(h):\n",
    "        for y in range(w):\n",
    "            i = image_stack[x, y, :]\n",
    "\n",
    "            if shadow_trick == True:\n",
    "                scriptI = np.diag(i)\n",
    "                g = np.linalg.lstsq(scriptI@scriptV, scriptI@i, rcond=None)[0]\n",
    "\n",
    "            else:\n",
    "                g = np.linalg.lstsq(scriptV, i, rcond=None)[0]\n",
    "\n",
    "\n",
    "            norm_g = np.linalg.norm(g)\n",
    "\n",
    "            albedo[x, y] = norm_g\n",
    "            normal[x, y,:] = g/norm_g if norm_g != 0 else 0\n",
    "\n",
    "    return albedo, normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2b8bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Question 1.1.1\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray5/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=True)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "show_results(albedo, normal, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeabe5b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1.1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18ac0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Question 1.1.2\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray25/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=True)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "show_results(albedo, normal, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053c7d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1.2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18e92",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Question 1.1.3\n",
    "\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray5/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=False)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "show_results(albedo, normal, height_map, SE)\n",
    "\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray25/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=False)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "show_results(albedo, normal, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bbd8c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1.3\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f4bb0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Test of Integrability (10pts)\n",
    "\n",
    "Before we can reconstruct the surface height map, it is required to compute the partial derivatives $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$ (or *p* and *q* in the algorithm). The partial derivatives also give us a chance to double check our computation, namely the test of *integrability*.\n",
    "\n",
    "1. Compute the partial derivatives (p and q in the algorithm) by filling in your code into **check_integrability()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec04d81",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_integrability(normals):\n",
    "    #  CHECK_INTEGRABILITY check the surface gradient is acceptable\n",
    "    #   normals: normal image\n",
    "    #   p : df / dx\n",
    "    #   q : df / dy\n",
    "    #   SE : Squared Errors of the 2 second derivatives\n",
    "\n",
    "    # initalization\n",
    "    p = np.zeros(normals.shape[:2])\n",
    "    q = np.zeros(normals.shape[:2])\n",
    "    SE = np.zeros(normals.shape[:2])\n",
    "\n",
    "    \"\"\"\n",
    "    Compute p and q, where\n",
    "    p measures value of df / dx\n",
    "    q measures value of df / dy\n",
    "\n",
    "    \"\"\"\n",
    "    p = np.divide(normals[:,:, 0], normals[:,:, 2])\n",
    "    q = np.divide(normals[:,:, 1], normals[:,:, 2])\n",
    "\n",
    "    # change nan to 0\n",
    "    p[p!=p] = 0\n",
    "    q[q!=q] = 0\n",
    "\n",
    "    \"\"\"\n",
    "    approximate second derivate by neighbor difference\n",
    "    and compute the Squared Errors SE of the 2 second derivatives SE\n",
    "\n",
    "    \"\"\"\n",
    "    h, w, _ = normals.shape\n",
    "\n",
    "    dpdy = np.diff(p, axis=1)\n",
    "    dpdy = np.hstack((dpdy, np.zeros((h, 1))))\n",
    "    dqdx = np.diff(q, axis=0)\n",
    "    dqdx = np.vstack((dqdx, np.zeros((1, w))))\n",
    "    SE = (dpdy - dqdx)**2\n",
    "\n",
    "    return p, q, SE\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    normals = normal\n",
    "    p, q, SE = check_integrability(normals)\n",
    "    plt.imshow(SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ef91f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Implement and compute the second derivatives according to the algorithm and perform the test of integrability by choosing a reasonable threshold. What could be the reasons for the errors? How does the test perform with different number of images used in the reconstruction process in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405747e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db53c92",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Shape by Integration (10pts)\n",
    "To reconstruct the surface height map, we need to continuously integrate the partial derivatives over a path. However, as we are working with discrete structures, you will be simply summing their values.\n",
    "\n",
    "The algorithm in the chapter presents a way to do the integration in column-major order, that is you start at the top-left corner and integrate along the first column, then go towards right along each row. Yet, it is also noticed that it would be better to use many different paths and average so as to spread around the errors in the derivative estimates.\n",
    "\n",
    "1. Construct the surface height map using column-major order as described in the algorithm, then implement row-major path integration. Your code should now go to **construct_surface()**.\n",
    "\n",
    "**Note**: By default, Numpy used row-major operations. So if you are unrolling an image to linearize the operation, you will end up with a row-major representation. Numpy can be configured to be column-major. Otherwise, if you are using the double for-loops without an unrolling operation, then this concern doesnâ€™t apply.\n",
    "\n",
    "**Hint**: You could further inspect the shape of the objects and normal directions by using **matplotlib.pyplot.quiver** function. You will have to choose appropriate sub-sampling ratios for proper illustration. You code goes to the **show_results()** in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df2f03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def construct_surface(p, q, path_type='column'):\n",
    "\n",
    "    '''\n",
    "    CONSTRUCT_SURFACE construct the surface function represented as height_map\n",
    "       p : measures value of df / dx\n",
    "       q : measures value of df / dy\n",
    "       path_type: type of path to construct height_map, either 'column',\n",
    "       'row', or 'average'\n",
    "       height_map: the reconstructed surface\n",
    "    '''\n",
    "\n",
    "    h, w = p.shape\n",
    "    height_map = np.zeros([h, w])\n",
    "\n",
    "    if path_type=='column':\n",
    "        for u in range(h):\n",
    "            for v in range(w):\n",
    "                height_map[u, v] = np.sum(p[0, :u]) + np.sum(q[1:v, u])\n",
    "\n",
    "    elif path_type=='row':\n",
    "        for u in range(h):\n",
    "            for v in range(w):\n",
    "                height_map[u, v] = np.sum(q[:v, 0]) + np.sum(p[v, 1:u])\n",
    "    elif path_type=='average':\n",
    "        for u in range(h):\n",
    "            for v in range(w):\n",
    "                height_map[u, v] = (np.sum(q[:v, 0]) + np.sum(p[v, 1:u])\n",
    "                                  + np.sum(p[0, :u]) + np.sum(q[1:v, u]))/2\n",
    "    return height_map\n",
    "\n",
    "hm = construct_surface(p, q, path_type='column')\n",
    "plt.imshow(hm)\n",
    "plt.colorbar()\n",
    "show_results(albedo, normal,  hm, SE)\n",
    "\n",
    "hm = construct_surface(p, q, path_type='row')\n",
    "plt.imshow(hm)\n",
    "plt.colorbar()\n",
    "show_results(albedo, normal,  hm, SE)\n",
    "\n",
    "hm = construct_surface(p, q, path_type='average')\n",
    "plt.imshow(hm)\n",
    "plt.colorbar()\n",
    "show_results(albedo, normal,  hm, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91557879",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. What are the differences in the results of the two paths?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca43502",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fac2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Now, take the average of the results. Do you see any improvement compared to when using only one path? Are the construction results different with different number of images being used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1639ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473a68",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Experiments with different objects (15pts)\n",
    "In this part, you will try to run the photometric stereo algorithm in various number of scenarios to see how well it can be generalized.\n",
    "\n",
    "1. Run the algorithm and show the results for the MonkeyGray model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288185bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def photometric_stereo(image_dir='./SphereGray5/' ):\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_syn_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface gradient from the stack of imgs and light source mat\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    [albedo, normals] = estimate_alb_nrm(image_stack, scriptV)\n",
    "\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "    [p, q, SE] = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface( p, q )\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "\n",
    "## Face\n",
    "def photometric_stereo_face(image_dir='./yaleB02/'):\n",
    "    [image_stack, scriptV] = load_face_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    albedo, normals = estimate_alb_nrm(image_stack, scriptV)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking')\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface(p, q )\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "    \n",
    "## Apple\n",
    "def photometric_stereo_apple(image_dir='./Apple_png/'):\n",
    "    [image_stack, scriptV] = load_apple_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    albedo, normals = estimate_alb_nrm(image_stack, scriptV)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking')\n",
    "    p, q, SE = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface(p, q )\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    photometric_stereo(image_dir='./images/photometrics_images/MonkeyGray/')\n",
    "    # photometric_stereo_face(image_dir='./images/photometrics_images/yaleB02/')\n",
    "    # photometric_stereo_apple(image_dir='./images/photometrics_images/Apple/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c44adc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. The albedo results of the monkey may comprise more albedo errors than in case of the sphere. Observe and describe the errors. What could be the reason for those errors? You may want to experiment with different number of images as you did in Question 1 to see the effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300d077",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9e17d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. How do you think that could help solving these errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378f616",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d05c2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So far, we have assumed that albedos are 1-channel grayscale images and that input images are also 1-channel. To work with 3-channel images, a simple solution is to split the input image into separate channels and treat them individually. Yet, that would generate a small problem while constructing the surface normal map if a pixel value in a channel is zero.\n",
    "\n",
    "1. Update the implementation to work for 3-channel RGB inputs and test it with 2 models SphereColor and MonkeyColor. \n",
    "2. Explain your changes and show your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da16d95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388254ec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Observe the problem in the constructed surface normal map and height map, explain why a zero pixel could be a problem and propose a way to overcome that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3405a82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d6338",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it's the time to try the algorithm on real-world datasets. For that purpose, we are going to use the [Yale Face Database](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html)\n",
    "\n",
    "1. Run the algorithm for the Yale Face images (included in the lab material). \n",
    "2. Observe and discuss the results for different integration paths. \n",
    "\n",
    "**Hint**: For proper computation of albedo and surface normal, you may want to suspend the shadow trick described in the text, and use the original formula:\n",
    "$$i = Vg(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df034f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4f6ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Discuss how the images violate the assumptions of the shape-from-shading methods. Remember to include specific input images to illustrate your points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e71de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc40200",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. How would the results improve when the problematic images are all removed? Try it out and show the results here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e1c89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dde1f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, show your results on real-world 3-channel RGB inputs, contained in the \"Apple\" folder, taken from [this dataset](http://vision.ucsd.edu/~nalldrin/research/cvpr08/datasets/) from the University of California San Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeaf07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Observe and discuss the results for different integration paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81943cea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Colour Spaces (20pts)\n",
    "\n",
    "In this part of the assignment, you will study the different colour spaces for image representations and experiment how to convert a given RGB image to a specific colour space.\n",
    "\n",
    "### 2.1 RGB Colour Model (3pts)\n",
    "\n",
    "Why do we use RGB colour model as a basis of our digital cameras and photography? How does a standard digital camera capture the full RGB colour image?\n",
    "\n",
    "### 2.2 Colour Space Conversion (10pts)\n",
    "\n",
    "Create a function to convert an RGB image into the following colour spaces by using the template code you are provided at the ConvertColourSpace function and other sub-functions. Visualize the new image and its channels separately in the same figure. That is, for example, in the case of HSV colour space, you need to visualize the converted HSV image, and its Hue, Saturation and Value channels separately (4 images, 1 figure). Do not change the already given code.\n",
    "\n",
    "__Opponent Colour Space__\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "O_1 \\\\\n",
    "O_2 \\\\\n",
    "O_3\n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "\\frac{R-G}{\\sqrt{2}} \\\\\n",
    "\\frac{R+G-2B}{\\sqrt{6}} \\\\\n",
    "\\frac{R+G+B}{\\sqrt{3}}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "__Normalized RGB (rgb) Colour Space__\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "r \\\\\n",
    "g \\\\\n",
    "b\n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "\\frac{R}{R+G+B} \\\\\n",
    "\\frac{G}{R+G+B} \\\\\n",
    "\\frac{B}{R+G+B}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "__HSV Colour Space__\n",
    "\n",
    "Convert the RGB image into HSV Colour Space. Use OpenCVâ€™s built-in function *cv2.cvtColor(img, cv2.RGB2HSV)*.\n",
    "\n",
    "__YCbCr Colour Space__\n",
    "\n",
    "Convert the RGB image into YCbCr Colour Space. Use OpenCVâ€™s built-in function *cv2.cvtColor(img, cv2.RGB2YCrCb)*. Note, you need to arrange the channels in $Y, C_b$ and $C_r$ order.\n",
    "\n",
    "__Grayscale__\n",
    "\n",
    "Convert the RGB image into grayscale by using 3 different methods mentioned in\n",
    "https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/\n",
    "In the end, check and report which method OpenCV uses for grayscale conversion, include it as well, and visualize all 4 in the same figure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63651c77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rgb2grays(input_image):\n",
    "    # converts an RGB into grayscale by using 4 different methods\n",
    "    RGB = getColourChannels(input_image)\n",
    "    new_image = []\n",
    "    \n",
    "    # ligtness method\n",
    "    new_image.append((np.maximum.reduce(RGB) + np.minimum.reduce(RGB)) / 2)\n",
    "    # average method\n",
    "    new_image.append(np.mean(RGB, axis=0))\n",
    "    # luminosity method\n",
    "    new_image.append((RGB[0]*0.21 + RGB[1]*0.72 + RGB[2]*0.7) / 3)\n",
    "    # built-in opencv function\n",
    "    new_image.append(cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY))\n",
    "#     print(new_image)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def rgb2opponent(input_image):\n",
    "    # converts an RGB image into opponent colour space\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def rgb2normedrgb(input_image):\n",
    "    # converts an RGB image into normalized rgb colour space\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def getColourChannels(input_image):\n",
    "\n",
    "    R = input_image[:, :, 0]\n",
    "    G = input_image[:, :, 1]\n",
    "    B = input_image[:, :, 2]\n",
    "\n",
    "    return [R, G, B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bd1b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(input_image):\n",
    "    # Fill in this function. Remember to remove the pass command\n",
    "    fig = plt.figure(figsize=(10,3))\n",
    "    for i in range(len(input_image)):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.imshow(input_image[i])\n",
    "#         print(input_image[i].shape)\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     plt.subplot(1,4,1)\n",
    "#     plt.imshow(input_image)\n",
    "# #     for i in getColourChannels(input_image):\n",
    "# #         plt.subplot(1,4,)\n",
    "#     plt.subplot(1,4,2)\n",
    "#     plt.imshow(input_image[:, :, 0])\n",
    "#     plt.subplot(1,4,3)\n",
    "#     plt.imshow(input_image[:, :, 1])\n",
    "#     plt.subplot(1,4,4)\n",
    "#     plt.imshow(input_image[:, :, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7b80c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ConvertColourSpace(input_image, colourspace):\n",
    "    '''\n",
    "    Converts an RGB image into a specified color space, visualizes the\n",
    "    color channels and returns the image in its new color space.\n",
    "\n",
    "    Colorspace options:\n",
    "      opponent\n",
    "      rgb -> for normalized RGB\n",
    "      hsv\n",
    "      ycbcr\n",
    "      gray\n",
    "\n",
    "    P.S: Do not forget the visualization part!\n",
    "    '''\n",
    "\n",
    "    # Convert the image into double precision for conversions\n",
    "    input_image = input_image.astype(np.float32)\n",
    "\n",
    "    if colourspace.lower() == 'opponent':\n",
    "        # fill in the rgb2opponent function\n",
    "        new_image = rgb2opponent(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'rgb':\n",
    "        # fill in the rgb2opponent function\n",
    "        new_image = rgb2normedrgb(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'hsv':\n",
    "        # use built-in function from opencv\n",
    "        pass\n",
    "\n",
    "    elif colourspace.lower() == 'ycbcr':\n",
    "        # use built-in function from opencv\n",
    "        pass\n",
    "\n",
    "    elif colourspace.lower() == 'gray':\n",
    "        # fill in the rgb2opponent function\n",
    "        new_image = rgb2grays(input_image)\n",
    "\n",
    "    else:\n",
    "        print('Error: Unknown colorspace type [%s]...' % colourspace)\n",
    "        new_image = input_image\n",
    "\n",
    "    visualize(new_image)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace the image name with a valid image\n",
    "    img_path = 'images/awb/awb.jpg'\n",
    "    # Read with opencv\n",
    "    I = cv2.imread(img_path)\n",
    "    # Convert from BGR to RGB\n",
    "    # This is a shorthand.\n",
    "    I = I[:, :, ::-1]\n",
    "\n",
    "    out_img = ConvertColourSpace(I, 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378a05e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.1**\n",
    "We use the RGB color model because it comprises the three primary additive colors, red, green and blue. With these three colors we can produce any color we want, including white, which occurs when all colors are mixed [Szeliski Book Fig. 2.8]. We use additive colors because the contrast they create with black screens works very well to produce digital images [1]. RGB is also convenient because it mimics the trichromatic nature of the human eye, which has different types of receptors called cones to sense red, green and blue [Lecture 2]. To capture a colored image, a standard digital camera is equipped with sensors that translate the wavelength of the photons to an RGB value. To achieve this, the sensors contain a Bayer Filter, which has the appearance of a checkboard with red, green and blue squares [Lecture 2].\n",
    "\n",
    "[1] https://medium.com/nerd-for-tech/the-color-theory-why-do-computers-use-rgb-instead-of-ryb-205b75d6e783\n",
    "\n",
    "**2.2**\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283cd2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Colour Space Properties (5pts)\n",
    "\n",
    "Explain each of those 5 colour spaces and their properties. What are the benefits of using a different colour space other than RGB? Provide reasons for each of the above cases. You can include your observations from the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae5311",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb04f78",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 More on Colour Spaces (2pts)\n",
    "\n",
    "Find one more colour space from the literature and simply explain its properties and give a use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b842e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b280abd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Intrinsic Image Decomposition (15pts)\n",
    "Intrinsic image decomposition is the process of separating an image into its formation components, such as reflectance (albedo) and shading (illumination). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Then, under the assumption of body (diffuse) reflection, linear sensor response and narrow band filters, the decomposition of the observed image $I(\\vec{x})$ at position $\\vec{x}$ can be approximated as the element-wise product of its albedo $R(\\vec{x})$ and shading $S(\\vec{x})$ intrinsics:\n",
    "\n",
    "$$I(\\vec{x})=R(\\vec{x}) \\times S(\\vec{x})$$\n",
    "\n",
    "In this part of the assignment, you will experiment with intrinsic image components to perform one of the computational photography applications; material recolouring. For the experiments, we will use images from a synthetic intrinsic image dataset. <a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2)\n",
    "\n",
    "<a name=\"cite_note-1\"></a><small>1. [^](#cite_ref-1) H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3-26, 1978.</small>\n",
    "\n",
    "<a name=\"cite_note-2\"></a><small>2. [^](#cite_ref-1) http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ef75c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. What other components can an image be decomposed other than albedo and shading Give an example and explain your reasoning. *(4pts)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e148a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Other components of an image can be shape and pose [Lecture 3 p.42]. For instance, in 3D reconstruction from images of different objects, to be able to recreate the object from its components, albedo and illumination is not sufficient. We also need information about the shape (vector field of surface normal of object) and pose (how the object is placed in the space) to synthesize the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d17f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. If you check the literature, you will see that almost all the intrinsic image decomposition datasets are composed of synthetic images. What might be the reason for that? *(2pts)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52379ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because it is very easy to model light in a 3D environment with artificial objects and capture millions of images automatically. Also, it is possible to extract the ground-truth intrinsic components of the object through the computer modeling software. On the other hand, capturing those images in different lighting conditions in real life and extracting the ground-truth intrinsic components is a very tedious and time-consuming manual process [Intrinsic Images in the Wild, Sean Bell et al.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ada20",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Show that you can actually reconstruct the original *turtle.png* image from its intrinsics using *turtle_albedo.png* and *turtle_shading.png*. In the end, your script should output a figure displaying the original image, its intrinsic images and the reconstructed one. Complete the code for function **iid_image_formation()**. *(4pts)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc5e72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def iid_image_formation(albedo_img, shading_img):\n",
    "    img = albedo_img * shading_img\n",
    "    return img\n",
    "\n",
    "img = plt.imread(os.path.join(os.getcwd(),'images','intrinsic_images','turtle.png'))\n",
    "albedo_img = plt.imread(os.path.join(os.getcwd(),'images','intrinsic_images','turtle_albedo.png'))\n",
    "shading_img = plt.imread(os.path.join(os.getcwd(),'images','intrinsic_images','turtle_shading.png'))\n",
    "prod_img = iid_image_formation(albedo_img,shading_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(20,3))\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(albedo_img)\n",
    "axs[1].set_title('Albedo image')\n",
    "axs[2].imshow(shading_img)\n",
    "axs[2].set_title('Shading image')\n",
    "axs[3].imshow(prod_img)\n",
    "axs[3].set_title('Reconstructed image')\n",
    "fig.suptitle('Fig 3.1: Image of a turtle and its components')\n",
    "for i in range(4):\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3db93",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Recoloring *(5pts)*\n",
    "Manipulating colours in photographs is an important problem with many applications in computer vision. Since the aim for recolouring algorithms is just to manipulate colours, better results can be obtained for such a task if the albedo image is available as it is independent of confounding illumination effects.\n",
    "\n",
    "Assume that you are given the *turtle.png* image and you have access to its\n",
    "intrinsic images *turtle_albedo.png* and *turtle_shading.png*.\n",
    "1. Find out the true material colour of the turtle in RGB space (which is uniform in this case).\n",
    "2. Recolour the turtle image with pure green (0, 255, 0). Display the original turtle image and the recoloured version on the same figure. Complete the code for function **recoloring()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e773cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def recoloring(albedo_img, shading_img):\n",
    "    recolored_albedo_img = albedo_img.copy()\n",
    "    old_color = albedo_img[int(albedo_img.shape[0]/2),int(albedo_img.shape[1]/2)]  # Get color of center pixel (non-black)\n",
    "    for i in range(albedo_img.shape[0]):\n",
    "        for j in range(albedo_img.shape[1]):\n",
    "            if all(albedo_img[i,j] != [0,0,0]):  # If pixel is non-black\n",
    "                recolored_albedo_img[i,j] = [0,1,0]  # Recolor pixel as pure green\n",
    "                recolored_img = iid_image_formation(recolored_albedo_img, shading_img)  # Get recolored image\n",
    "    return old_color, recolored_img\n",
    "\n",
    "old_color, recolored_img, = recoloring(albedo_img, shading_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(recolored_img)\n",
    "axs[1].set_title('Recolored image')\n",
    "\n",
    "fig.suptitle(f'Fig 3.2: Image of a turtle and its recoloring from {tuple([int(i) for i in 255*old_color])} to (0,255,0)')\n",
    "for i in range(2):\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a94059",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Although you have recoloured the object with pure green, the reconstructed images do not seem to display those pure colors and thus the colour distributions over the object do not appear uniform. Explain the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04800720",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This happens because of the multiplication of the albedo matrix with the shading matrix. The shading matrix has non-1 values, which means that, when multiplying it with the pure green color of the albedo, some pixels will have green tones of different lightness depending on the value of the shading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4721fe6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that this was a simple case where the image is synthetic, object centered and has only one colour, and you have access to its ground-truth intrinsic images. Real world scenarios require more than just replacing a single colour with another, not to mention the complexity of achieving a decent intrinsic image decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca6266",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 Colour Constancy (15pts)\n",
    "\n",
    "Colour constancy is the ability to perceive colors of objects, invariant to the colour of the light source. The aim for colour constancy algorithms is first to estimate the illuminant of the light source, and then correct the image so that the corrected image appears to be taken under a canonical (white) light source. The task of the automatic white balance (AWB) is to do the same in digital cameras so that the images taken by a digital camera look as natural as possible.\n",
    "\n",
    "In this part of the assignment, you will implement the most famous colour constancy algorithm; *Grey-World Algorithm*.\n",
    "\n",
    "### Grey-World Algorithm\n",
    "The algorithm assumes that, under a white light source, the average colour in a scene should be achromatic (grey, [128, 128, 128]).\n",
    "\n",
    "1. Complete the function to apply colour correction to an RGB image by using Grey-World algorithm. Display the original image and the colour corrected one on the same figure. Use awb.jpg image to test your algorithm. In the end, you should see that the reddish colour cast on the image is removed and it looks more natural.\n",
    "\n",
    "  ***Note:*** You do not need to apply any pre or post processing steps. For the calculation or processing, you are not allowed to use any available code or any dedicated library function except *standard Numpy functions*.\n",
    "\n",
    "   ***Hint:*** Check the von Kries model for this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb068f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def grey_world(awb_img):\n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85defe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Give an example case for Grey-World Algorithm on where it might fail. Remember to include your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c6af9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbea98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Find out one more colour constancy algorithms from the literature and explain it briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f2670",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
