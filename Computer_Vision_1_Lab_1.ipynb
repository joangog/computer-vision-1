{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2e566f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/261c3e30bae8b8bdc83541ffa9331b52fcf015e6/3-Figure2-1.png\" width=50% >\n",
    "\n",
    "# <center> Assignment 1: Photometric Stereo & Colour </center>\n",
    "<center> Computer Vision 1 University of Amsterdam </center>\n",
    "    <center> Due 23:59 PM, September 17, 2022 (Amsterdam time) </center>\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e018a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## General guidelines\n",
    "Your code and discussion must be submitted through this jupyter notebook, renamed to **13486578_12167320_14334739.ipynb** before the deadline by submitting it to the Canvas Lab 1 Assignment. For full credit, make sure your answer follows these guidelines:\n",
    "- Please express your thoughts concisely. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make your notebook self-contained and complete.\n",
    "- Analyze your results and discuss them, e.g. why algorithm A works better than algorithm B in a certain problem.\n",
    "- Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAsâ€™ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90915de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0 Environment Set Up\n",
    "\n",
    "You are allowed to use **only** the following library versions:\n",
    "- python=3.10.4\n",
    "- matplotlib==3.5.3\n",
    "- matplotlib-inline==0.1.6\n",
    "- numpy==1.23.2\n",
    "- opencv-python==4.6.0.66\n",
    "\n",
    "Using functions that are not working in these versions could lead to grade deduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cd538",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Photometric Stereo (50pts)\n",
    "\n",
    "In this part of the assignment, you are going to implement the photometric stereo algorithm as described in Section 5.4 (Forsyth and Ponce, *Computer Vision: A Modern Approach*). The chapter snippet can be found in the course materials.\n",
    "\n",
    "Following this instruction, you will have to edit and fill in your code in the functions **estimate_alb_nrm**, **check_integrability**, and **construct_surface**. The main function **photometric_stereo** is provided for reference and should not be taken as is. Throughout the assignment, you will be asked to perform different trials and experiments which will require you to adjust the main code accordingly, this also shows how well you can cope with the materials.\n",
    "\n",
    "Include images of the results into this notebook. For 3D models, make sure to choose a viewpoint that makes the structure as clear as possible and/or feel free to take them from multiple viewpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7417df7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Estimating Albedo and Surface Normal (15pts)\n",
    "Let us start with the grayscale sphere model, which is located in the SphereGray5 folder. The folder contains 5 images of a sphere with grayscale checker texture under similar lighting conditions with the one in the book. Your task is to estimate the surface reflectance (albedo) and surface normal of this model. The light source directions are encoded in the image file names.\n",
    "\n",
    "1. Complete the code for function **estimate_alb_nrm()** to estimate albedo and surface normal map for the SphereGray5 folder. What do you expect to see in albedo image and how is it different with your result?\n",
    "2. In principle, what is the minimum number of images you need to estimate albedo and surface normal? Run the algorithm with more images by using SphereGray25 and observe the differences in the results. You could try all images at once or a few at the time, in an incremental fashion. Choose a strategy and justify it by discussing your results.\n",
    "3. What is the impact of shadows in photometric stereo? Explain the trick that is used in the text to deal with shadows. Remove that trick and check your results. Is the trick necessary in the case of 5 images, how about 25 images?\n",
    "\n",
    "**Hint**: To get the least-squares solution of a linear system, you can use **numpy.linalg.lstsq** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afad39b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First some utils we need\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def load_syn_images(image_dir='./SphereGray5/', channel=0):\n",
    "    files = os.listdir(image_dir)\n",
    "    #files = [os.path.join(image_dir, f) for f in files]\n",
    "    nfiles = len(files)\n",
    "\n",
    "    image_stack = None\n",
    "    V = 0\n",
    "    Z = 0.5\n",
    "\n",
    "    for i in range(nfiles):\n",
    "        # read input image\n",
    "        im = cv2.imread(os.path.join(image_dir, files[i]))\n",
    "        im = im[:,:,channel]\n",
    "\n",
    "        # stack at third dimension\n",
    "        if image_stack is None:\n",
    "            h, w = im.shape\n",
    "            print('Image size (H*W): %d*%d' %(h,w) )\n",
    "            image_stack = np.zeros([h, w, nfiles], dtype=int)\n",
    "            V = np.zeros([nfiles, 3], dtype=np.float64)\n",
    "\n",
    "        image_stack[:,:,i] = im\n",
    "\n",
    "        # read light direction from image name\n",
    "        X = np.double(files[i][(files[i].find('_')+1):files[i].rfind('_')])\n",
    "        Y = np.double(files[i][files[i].rfind('_')+1:files[i].rfind('.png')])\n",
    "        V[i, :] = [-X, Y, Z]\n",
    "\n",
    "    # normalization\n",
    "    image_stack = np.double(image_stack)\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val) if max_val != min_val else image_stack  #  Fixed for when max_val == min_val\n",
    "    normV = np.tile(np.sqrt(np.sum(V ** 2, axis=1, keepdims=True)), (1, V.shape[1]))\n",
    "    scriptV = V / normV\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def load_face_images(image_dir='./yaleB02/', remove_problematic=False, problematic_images=[]):\n",
    "    num_prob = len(problematic_images)\n",
    "    num_images = 64 - num_prob\n",
    "    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n",
    "    ambient_image = cv2.imread(filename, -1)\n",
    "    h, w = ambient_image.shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob, random\n",
    "    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n",
    "    if remove_problematic:  # remove problematic images\n",
    "        for img in problematic_images:\n",
    "            d.remove(os.path.join(image_dir,img))\n",
    "\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    for j in range(num_images):\n",
    "        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n",
    "\n",
    "\n",
    "    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n",
    "    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n",
    "    z = np.sin(np.pi*ang[1,:]/180)\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def load_apple_images(image_dir='./Apples_png/'):\n",
    "    num_images = 100\n",
    "    filename = os.path.join(image_dir, 'I_0000.png')\n",
    "    try_image = cv2.imread(filename, -1)\n",
    "    h, w = try_image[:,:,0].shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob\n",
    "    d = glob.glob(os.path.join(image_dir, 'I_0*.png'))\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "    filenames_idx = []\n",
    "    for i in filenames:\n",
    "        filenames_idx.append(int(i.split('_')[1].split('.')[0]))\n",
    "\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    for j in range(num_images):\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,0]\n",
    "\n",
    "    with open('./images/photometrics_images/Apple/light_directions_refined.txt') as file:\n",
    "        lines = [line.split() for line in file]\n",
    "        x, y, z = [], [], []\n",
    "        for idx in filenames_idx:\n",
    "            x.append(float(lines[idx][0]))\n",
    "            y.append(float(lines[idx][1]))\n",
    "            z.append(float(lines[idx][2]))\n",
    "\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def show_results(albedo, normals, height_map, SE):\n",
    "    # Stride in the plot, you may want to adjust it to different images\n",
    "    stride = 1\n",
    "\n",
    "    # showing albedo map\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Albedo map')\n",
    "    albedo_max = albedo.max()\n",
    "    albedo_max = 1\n",
    "    albedo = albedo / albedo_max\n",
    "    if albedo.ndim == 2:\n",
    "        plt.imshow(albedo, cmap=\"gray\")\n",
    "    elif albedo.ndim == 3:\n",
    "        plt.imshow(albedo)\n",
    "    plt.show()\n",
    "\n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure(figsize=(8,2))\n",
    "    ax1 = figure.add_subplot(131)\n",
    "    ax1.imshow(normals[..., 0])\n",
    "    ax2 = figure.add_subplot(132)\n",
    "    ax2.imshow(normals[..., 1])\n",
    "    ax3 = figure.add_subplot(133)\n",
    "    ax3.imshow(normals[..., 2])\n",
    "    figure.suptitle('Surface normals as three separate channels')\n",
    "    plt.show()\n",
    "\n",
    "    # meshgrid\n",
    "    X, Y, _ = np.meshgrid(np.arange(0,np.shape(normals)[0], stride),\n",
    "    np.arange(0,np.shape(normals)[1], stride),\n",
    "    np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "\n",
    "    '''\n",
    "    =============\n",
    "    You could further inspect the shape of the objects and normal directions by using plt.quiver() function.\n",
    "    =============\n",
    "    '''\n",
    "\n",
    "    # plotting the SE\n",
    "    H = SE[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.title('Squared Error')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting model geometry\n",
    "    H = height_map[::stride,::stride]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y, H.T)\n",
    "    plt.title('Height Map')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5f1de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_alb_nrm( image_stack, scriptV, shadow_trick=True):\n",
    "\n",
    "    # COMPUTE_SURFACE_GRADIENT compute the gradient of the surface\n",
    "    # INPUT:\n",
    "    # image_stack : the images of the desired surface stacked up on the 3rd dimension\n",
    "    # scriptV : matrix V (in the algorithm) of source and camera information\n",
    "    # shadow_trick: (true/false) whether or not to use shadow trick in solving linear equations\n",
    "    # OUTPUT:\n",
    "    # albedo : the surface albedo\n",
    "    # normal : the surface normal\n",
    "\n",
    "    h, w, _ = image_stack.shape\n",
    "\n",
    "    # create arrays for\n",
    "    # albedo (1 channel)\n",
    "    # normal (3 channels)\n",
    "    albedo = np.zeros([h, w])\n",
    "    normal = np.zeros([h, w, 3])\n",
    "\n",
    "    \"\"\"\n",
    "    for each point in the image array\n",
    "        stack image values into a vector i\n",
    "        construct the diagonal matrix scriptI\n",
    "        solve scriptI * scriptV * g = scriptI * i to obtain g for this point\n",
    "        albedo at this point is |g|\n",
    "        normal at this point is g / |g|\n",
    "    \"\"\"\n",
    "    for x in range(h):\n",
    "        for y in range(w):\n",
    "            i = image_stack[x, y, :]\n",
    "\n",
    "            if shadow_trick == True:\n",
    "                scriptI = np.diag(i)\n",
    "                g = np.linalg.lstsq(scriptI@scriptV, scriptI@i, rcond=None)[0]\n",
    "\n",
    "            else:\n",
    "                g = np.linalg.lstsq(scriptV, i, rcond=None)[0]\n",
    "\n",
    "\n",
    "            norm_g = np.linalg.norm(g)\n",
    "\n",
    "            albedo[x, y] = norm_g\n",
    "            normal[x, y,:] = g/norm_g if norm_g != 0 else 0\n",
    "\n",
    "    return albedo, normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d350c6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Question 1.1.1\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray5/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=True)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "\n",
    "#  We don't need SE and height map yet so just use a zero placeholder\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "\n",
    "show_results(albedo, normal, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baaface",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1.1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8182f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Question 1.1.2\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray25/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=True)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "\n",
    "#  We don't need SE and height map yet so just use a zero placeholder\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "\n",
    "show_results(albedo, normal, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d5d7e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1.2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e536e5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  Question 1.1.3\n",
    "\n",
    "\n",
    "# SphereGray5\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray5/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=False)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "\n",
    "#  We don't need SE and height map yet so just use a zero placeholder\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "\n",
    "show_results(albedo, normal, height_map, SE)\n",
    "\n",
    "\n",
    "# SphereGray25\n",
    "image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray25/')\n",
    "albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=False)\n",
    "h, w = image_stack.shape[0], image_stack.shape[1]\n",
    "\n",
    "#  We don't need SE and height map yet so just use a zero placeholder\n",
    "SE = np.zeros(normal.shape[:2])\n",
    "height_map = np.zeros([h, w])\n",
    "\n",
    "show_results(albedo, normal, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe5e93",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1.3\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100c591",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Test of Integrability (10pts)\n",
    "\n",
    "Before we can reconstruct the surface height map, it is required to compute the partial derivatives $\\frac{\\delta f}{\\delta x}$ and $\\frac{\\delta f}{\\delta y}$ (or *p* and *q* in the algorithm). The partial derivatives also give us a chance to double check our computation, namely the test of *integrability*.\n",
    "\n",
    "1. Compute the partial derivatives (p and q in the algorithm) by filling in your code into **check_integrability()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c7bbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_integrability(normals):\n",
    "    #  CHECK_INTEGRABILITY check the surface gradient is acceptable\n",
    "    #   normals: normal image\n",
    "    #   p : df / dx\n",
    "    #   q : df / dy\n",
    "    #   SE : Squared Errors of the 2 second derivatives\n",
    "\n",
    "    # initalization\n",
    "    p = np.zeros(normals.shape[:2])\n",
    "    q = np.zeros(normals.shape[:2])\n",
    "    SE = np.zeros(normals.shape[:2])\n",
    "\n",
    "    \"\"\"\n",
    "    Compute p and q, where\n",
    "    p measures value of df / dx\n",
    "    q measures value of df / dy\n",
    "\n",
    "    \"\"\"\n",
    "    p = np.divide(normals[:,:, 0], normals[:,:, 2])\n",
    "    q = np.divide(normals[:,:, 1], normals[:,:, 2])\n",
    "\n",
    "    # change nan to 0\n",
    "    p[p!=p] = 0\n",
    "    q[q!=q] = 0\n",
    "\n",
    "    \"\"\"\n",
    "    approximate second derivate by neighbor difference\n",
    "    and compute the Squared Errors SE of the 2 second derivatives SE\n",
    "\n",
    "    \"\"\"\n",
    "    h, w, _ = normals.shape\n",
    "\n",
    "    dpdy = np.diff(p, axis=1)\n",
    "    dpdy = np.hstack((dpdy, np.zeros((h, 1))))\n",
    "    dqdx = np.diff(q, axis=0)\n",
    "    dqdx = np.vstack((dqdx, np.zeros((1, w))))\n",
    "    SE = (dpdy - dqdx)**2\n",
    "\n",
    "    return p, q, SE\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    image_stack, scriptV = load_syn_images(image_dir='./images/photometrics_images/SphereGray5/')\n",
    "    albedo, normal = estimate_alb_nrm(image_stack, scriptV, shadow_trick=True)\n",
    "    p, q, SE = check_integrability(normal)\n",
    "    plt.imshow(SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe35b17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Implement and compute the second derivatives according to the algorithm and perform the test of integrability by choosing a reasonable threshold. What could be the reasons for the errors? How does the test perform with different number of images used in the reconstruction process in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f0518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b436bcc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Shape by Integration (10pts)\n",
    "To reconstruct the surface height map, we need to continuously integrate the partial derivatives over a path. However, as we are working with discrete structures, you will be simply summing their values.\n",
    "\n",
    "The algorithm in the chapter presents a way to do the integration in column-major order, that is you start at the top-left corner and integrate along the first column, then go towards right along each row. Yet, it is also noticed that it would be better to use many different paths and average so as to spread around the errors in the derivative estimates.\n",
    "\n",
    "1. Construct the surface height map using column-major order as described in the algorithm, then implement row-major path integration. Your code should now go to **construct_surface()**.\n",
    "\n",
    "**Note**: By default, Numpy used row-major operations. So if you are unrolling an image to linearize the operation, you will end up with a row-major representation. Numpy can be configured to be column-major. Otherwise, if you are using the double for-loops without an unrolling operation, then this concern doesnâ€™t apply.\n",
    "\n",
    "**Hint**: You could further inspect the shape of the objects and normal directions by using **matplotlib.pyplot.quiver** function. You will have to choose appropriate sub-sampling ratios for proper illustration. You code goes to the **show_results()** in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3061a70",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def construct_surface(p, q, path_type='average'):\n",
    "\n",
    "    '''\n",
    "    CONSTRUCT_SURFACE construct the surface function represented as height_map\n",
    "       p : measures value of df / dx\n",
    "       q : measures value of df / dy\n",
    "       path_type: type of path to construct height_map, either 'column',\n",
    "       'row', or 'average'\n",
    "       height_map: the reconstructed surface\n",
    "    '''\n",
    "\n",
    "    h, w = p.shape\n",
    "    height_map = np.zeros([h, w])\n",
    "\n",
    "    if path_type=='column':\n",
    "        for u in range(1,h):\n",
    "            height_map[u,0] = height_map[u-1,0] + q[u,0]\n",
    "        for u in range(0,h):\n",
    "            for v in range(1,w):\n",
    "                height_map[u,v] = height_map[u,v-1] + p[u,v]\n",
    "\n",
    "    elif path_type=='row':\n",
    "        for v in range(1,w):\n",
    "            height_map[0,v] = height_map[0,v-1] + p[0,v]\n",
    "        for v in range(0,w):\n",
    "            for u in range(1,h):\n",
    "                height_map[u,v] = height_map[u-1,v] + q[u,v]\n",
    "\n",
    "    elif path_type=='average':\n",
    "        height_map_col = np.zeros([h, w])\n",
    "        height_map_row = np.zeros([h, w])\n",
    "        for u in range(1,h):\n",
    "            height_map_col[u,0] = height_map_col[u-1,0] + q[u,0]\n",
    "        for u in range(0,h):\n",
    "            for v in range(1,w):\n",
    "                height_map_col[u,v] = height_map_col[u,v-1] + p[u,v]\n",
    "        for v in range(1,w):\n",
    "            height_map_row[0,v] = height_map_row[0,v-1] + p[0,v]\n",
    "        for v in range(0,w):\n",
    "            for u in range(1,h):\n",
    "                height_map_row[u,v] = height_map_row[u-1,v] + q[u,v]\n",
    "        height_map = height_map_col + height_map_row / 2\n",
    "\n",
    "    return height_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a733fa9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hm = construct_surface(p, q, path_type='column')\n",
    "plt.imshow(hm)\n",
    "plt.colorbar()\n",
    "show_results(albedo, normal,  hm, SE)\n",
    "\n",
    "hm = construct_surface(p, q, path_type='row')\n",
    "plt.imshow(hm)\n",
    "plt.colorbar()\n",
    "show_results(albedo, normal,  hm, SE)\n",
    "\n",
    "hm = construct_surface(p, q, path_type='average')\n",
    "plt.imshow(hm)\n",
    "plt.colorbar()\n",
    "show_results(albedo, normal,  hm, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348fdf1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. What are the differences in the results of the two paths?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe5056",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569de999",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Now, take the average of the results. Do you see any improvement compared to when using only one path? Are the construction results different with different number of images being used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683044f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e6b65",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Experiments with different objects (15pts)\n",
    "In this part, you will try to run the photometric stereo algorithm in various number of scenarios to see how well it can be generalized.\n",
    "\n",
    "1. Run the algorithm and show the results for the MonkeyGray model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def photometric_stereo_helpfun(image_stack, scriptV, shadow_trick=True, path_type='column'):  # Helper function for the following photometric_stereo functions to avoid code repetition\n",
    "    # N = 5  #  Limit the number of images to N\n",
    "    # image_stack = image_stack[:,:,:N]\n",
    "    # scriptV = scriptV[:N]\n",
    "\n",
    "    [h, w, n] = image_stack.shape\n",
    "\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface gradient from the stack of imgs and light source mat\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "    [albedo, normals] = estimate_alb_nrm(image_stack, scriptV, shadow_trick)\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "    [p, q, SE] = check_integrability(normals)\n",
    "\n",
    "    threshold = 0.005;\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan') # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    height_map = construct_surface( p, q, path_type=path_type)\n",
    "\n",
    "    # show results\n",
    "    show_results(albedo, normals, height_map, SE)\n",
    "\n",
    "def photometric_stereo(image_dir='./SphereGray5/', channel=0, shadow_trick=True):\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_syn_images(image_dir, channel=channel)\n",
    "\n",
    "    photometric_stereo_helpfun(image_stack, scriptV, shadow_trick)\n",
    "\n",
    "## Face\n",
    "def photometric_stereo_face(image_dir='./yaleB02/', shadow_trick=True, path_type='column', remove_problematic=False, problematic_images=[]):\n",
    "    [image_stack, scriptV] = load_face_images(image_dir, remove_problematic, problematic_images)\n",
    "    photometric_stereo_helpfun(image_stack, scriptV, shadow_trick, path_type)\n",
    "\n",
    "## Apple\n",
    "def photometric_stereo_apple(image_dir='./Apple_png/', shadow_trick=True, path_type='column'):\n",
    "    [image_stack, scriptV] = load_apple_images(image_dir)\n",
    "    photometric_stereo_helpfun(image_stack, scriptV, shadow_trick, path_type)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "photometric_stereo(image_dir='./images/photometrics_images/MonkeyGray/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. The albedo results of the monkey may comprise more albedo errors than in case of the sphere. Observe and describe the errors. What could be the reason for those errors? You may want to experiment with different number of images as you did in Question 1 to see the effects."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that most of the errors occur on the parts of the object that cannot be seen fully from the point of view that the image depicts. For instance, in the case of the monkey, only the front faces of the object can be seen fully but the side faces cannot and therefore cause the most errors when generating the surface. Compared to the monkey, the sphere's shape has less \"hidden faces\" and this is the reason there are fewer errors. The reason for the errors is that, since the side faces cannot be seen properly, the photometric images contain less information about them compared to the front faces. Thus, the albedo and 3d reconstruction of the surface is more erroneous."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. How do you think that could help solving these errors?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By increasing the number of images, we can see the object in more lighting conditions. This increases the information we have about the surface of the object. Therefore, the albedo and surface produced becomes more accurate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far, we have assumed that albedos are 1-channel grayscale images and that input images are also 1-channel. To work with 3-channel images, a simple solution is to split the input image into separate channels and treat them individually. Yet, that would generate a small problem while constructing the surface normal map if a pixel value in a channel is zero.\n",
    "\n",
    "1. Update the implementation to work for 3-channel RGB inputs and test it with 2 models SphereColor and MonkeyColor.\n",
    "2. Explain your changes and show your results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  SphereColor for each RGB channel individually\n",
    "channels = 3  # Number of channels\n",
    "for c in range(channels):\n",
    "    print(f'RESULTS FOR CHANNEL {c} ----------------------------------------------------------------------------')\n",
    "    photometric_stereo(image_dir='./images/photometrics_images/SphereColor/', channel=c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  MonkeyColor for each RGB channel individually\n",
    "channels = 3  # Number of channels\n",
    "for c in range(channels):\n",
    "    print(f'RESULTS FOR CHANNEL {c} ----------------------------------------------------------------------------')\n",
    "    photometric_stereo(image_dir='./images/photometrics_images/MonkeyColor/', channel=c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simply, for each channel we execute the photometric_stereo function with the specific channel index. To do this, we added the new argument \"channel\" to the photometric_stereo function. Also, to avoid code repetition among all photometric_stereo functions, we added the new helper function photometric_stereo_helpfun. However, treating the RGB channels individually causes the problem that we explain below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "ec51c298",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Observe the problem in the constructed surface normal map and height map, explain why a zero pixel could be a problem and propose a way to overcome that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce41e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The problem in the results is that, when treating the channels individually, a pixel that has a 0 value for the specific color channel will appear as if it is part of the black background or, more intuitively, that the shape has a \"hole\", even though that is not the case. A way to overcome that would be to stack all RGB channels of all images together in one stack and process that as a whole. The code for doing that can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_syn_images_RGB(image_dir='./SphereGray5/'):\n",
    "    files = os.listdir(image_dir)\n",
    "    #files = [os.path.join(image_dir, f) for f in files]\n",
    "    nfiles = len(files)\n",
    "\n",
    "    image_stack = None\n",
    "    V = 0\n",
    "    Z = 0.5\n",
    "\n",
    "    for i in range(nfiles):\n",
    "        # read input image\n",
    "        im = cv2.imread(os.path.join(image_dir, files[i]))\n",
    "        im = im[:,:,:]\n",
    "\n",
    "        # stack at third dimension\n",
    "        if image_stack is None:\n",
    "            h, w, c = im.shape\n",
    "            print('Image size (H*W): %d*%d' %(h,w) )\n",
    "            image_stack = np.zeros([h, w, nfiles*c], dtype=int)  # nfiles*c for stacking all RGB channels of all images in one stack\n",
    "            V = np.zeros([nfiles*c, 3], dtype=np.float64)\n",
    "\n",
    "        image_stack[:,:,i*c:i*c+c] = im #  Stack all RGB channels of all images in one stack\n",
    "\n",
    "        # read light direction from image name\n",
    "        X = np.double(files[i][(files[i].find('_')+1):files[i].rfind('_')])\n",
    "        Y = np.double(files[i][files[i].rfind('_')+1:files[i].rfind('.png')])\n",
    "        V[i*c:i*c+c, :] = [-X, Y, Z]\n",
    "\n",
    "    # normalization\n",
    "    image_stack = np.double(image_stack)\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val) if max_val != min_val else image_stack  #  Fixed for when max_val == min_val\n",
    "    normV = np.tile(np.sqrt(np.sum(V ** 2, axis=1, keepdims=True)), (1, V.shape[1]))\n",
    "    scriptV = V / normV\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "\n",
    "def photometric_stereo_RGB(image_dir='./SphereGray5/', shadow_trick=True):\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_syn_images_RGB(image_dir)\n",
    "    photometric_stereo_helpfun(image_stack, scriptV, shadow_trick)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SphereColor with RGB fix\n",
    "photometric_stereo_RGB(image_dir='./images/photometrics_images/SphereColor/')\n",
    "\n",
    "# MonkeyColor with RGB fix\n",
    "photometric_stereo_RGB(image_dir='./images/photometrics_images/MonkeyColor/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "742cc69e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it's the time to try the algorithm on real-world datasets. For that purpose, we are going to use the [Yale Face Database](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html)\n",
    "\n",
    "1. Run the algorithm for the Yale Face images (included in the lab material). \n",
    "2. Observe and discuss the results for different integration paths. \n",
    "\n",
    "**Hint**: For proper computation of albedo and surface normal, you may want to suspend the shadow trick described in the text, and use the original formula:\n",
    "$$i = Vg(x,y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5705c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# yaleB02 with column integration\n",
    "photometric_stereo_face(image_dir='./images/photometrics_images/yaleB02/', shadow_trick=False, path_type='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4774f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# yaleB02 with row integration\n",
    "photometric_stereo_face(image_dir='./images/photometrics_images/yaleB02/', shadow_trick=False, path_type='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f36e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# yaleB02 with average integration\n",
    "photometric_stereo_face(image_dir='./images/photometrics_images/yaleB02/', shadow_trick=False, path_type='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ac10f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The best visual result occurs from the average path integration. With column path integration it seems that the top and bottom of the face are tilted upwards, whereas with row path integration the left and right sides of the face are tilted upwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baddecb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Discuss how the images violate the assumptions of the shape-from-shading methods. Remember to include specific input images to illustrate your points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ca037",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First of all, the object of the images is not convex as it has concave parts such as between the nose and the cheeks.\n",
    "Secondly, the assumption of a distant observer is violated as the face is very close to the camera.\n",
    "Thirdly, the object is not smooth as the skin is textured. Also, in the dataset there are two images that appear to have some form of lined texture. These are \"yaleB02_P00A+020E-10.pgm\" and \"yaleB02_P00A+025E+00.pgm\". We remove these with the following code and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "photometric_stereo_face(image_dir='./images/photometrics_images/yaleB02/', shadow_trick=False, path_type='average', remove_problematic=True, problematic_images=[\"yaleB02_P00A+020E-10.pgm\",\"yaleB02_P00A+025E+00.pgm\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "fccb2013",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. How would the results improve when the problematic images are all removed? Try it out and show the results here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01e092",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We expect that the shape will be more accurate to the real object. However, we don't see an apparent difference as the only problematic images are just two and therefore they do not affect the shape in a considerably harmful way when they are included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec8bb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, show your results on real-world 3-channel RGB inputs, contained in the \"Apple\" folder, taken from [this dataset](http://vision.ucsd.edu/~nalldrin/research/cvpr08/datasets/) from the University of California San Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b96326",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Observe and discuss the results for different integration paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def load_apple_images_RGB(image_dir='./Apples_png/'):\n",
    "    num_images = 100\n",
    "    filename = os.path.join(image_dir, 'I_0000.png')\n",
    "    try_image = cv2.imread(filename, -1)\n",
    "    h, w, c = try_image[:,:,:].shape\n",
    "\n",
    "    # get list of all other image files\n",
    "    import glob\n",
    "    d = glob.glob(os.path.join(image_dir, 'I_0*.png'))\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "    filenames_idx = []\n",
    "    for i in filenames:\n",
    "        filenames_idx.append(int(i.split('_')[1].split('.')[0]))\n",
    "\n",
    "    ang = np.zeros([2, num_images*c])\n",
    "    image_stack = np.zeros([h, w, num_images*c])  # nfiles*c for stacking all RGB channels of all images in one stack\n",
    "\n",
    "    for j in range(num_images):\n",
    "        image_stack[...,j*c:+j*c+c] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,:]  #  Stack all RGB channels of all images in one stack\n",
    "\n",
    "    with open('./images/photometrics_images/Apple/light_directions_refined.txt') as file:\n",
    "        lines = [line.split() for line in file]\n",
    "        x, y, z = [], [], []\n",
    "        for idx in filenames_idx:\n",
    "            for c_i in range(c):  # for each channel (use the same x,y,z)\n",
    "                x.append(float(lines[idx][0]))\n",
    "                y.append(float(lines[idx][1]))\n",
    "                z.append(float(lines[idx][2]))\n",
    "\n",
    "    scriptV = np.array([y,z,x]).transpose(1,0)\n",
    "\n",
    "    image_stack = np.double(image_stack)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "\n",
    "    return image_stack, scriptV\n",
    "\n",
    "## Apple\n",
    "def photometric_stereo_apple_RGB(image_dir='./Apple_png/', shadow_trick=True, path_type='column'):\n",
    "    [image_stack, scriptV] = load_apple_images_RGB(image_dir)\n",
    "    photometric_stereo_helpfun(image_stack, scriptV, shadow_trick, path_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298aa572",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Apple with column integration\n",
    "photometric_stereo_apple_RGB(image_dir='./images/photometrics_images/Apple/', shadow_trick=False, path_type='column')\n",
    "\n",
    "# Apple with row integration\n",
    "photometric_stereo_apple_RGB(image_dir='./images/photometrics_images/Apple/', shadow_trick=False, path_type='row')\n",
    "\n",
    "# Apple with average integration\n",
    "photometric_stereo_apple_RGB(image_dir='./images/photometrics_images/Apple/', shadow_trick=False, path_type='average')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that for all cases the surface is not reconstructed properly. This probably means that the algorithm is not suited for problems like the apple dataset. As with all the other datasets, with the column integration the shape is sliced horizontally, with the row integration it is sliced vertically, and finally with the average of the two we create a mixture of both surfaces."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "c9bdd6f9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Colour Spaces (20pts)\n",
    "\n",
    "In this part of the assignment, you will study the different colour spaces for image representations and experiment how to convert a given RGB image to a specific colour space.\n",
    "\n",
    "### 2.1 RGB Colour Model (3pts)\n",
    "\n",
    "Why do we use RGB colour model as a basis of our digital cameras and photography? How does a standard digital camera capture the full RGB colour image?\n",
    "\n",
    "### 2.2 Colour Space Conversion (10pts)\n",
    "\n",
    "Create a function to convert an RGB image into the following colour spaces by using the template code you are provided at the ConvertColourSpace function and other sub-functions. Visualize the new image and its channels separately in the same figure. That is, for example, in the case of HSV colour space, you need to visualize the converted HSV image, and its Hue, Saturation and Value channels separately (4 images, 1 figure). Do not change the already given code.\n",
    "\n",
    "__Opponent Colour Space__\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "O_1 \\\\\n",
    "O_2 \\\\\n",
    "O_3\n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "\\frac{R-G}{\\sqrt{2}} \\\\\n",
    "\\frac{R+G-2B}{\\sqrt{6}} \\\\\n",
    "\\frac{R+G+B}{\\sqrt{3}}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "__Normalized RGB (rgb) Colour Space__\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "r \\\\\n",
    "g \\\\\n",
    "b\n",
    "\\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "\\frac{R}{R+G+B} \\\\\n",
    "\\frac{G}{R+G+B} \\\\\n",
    "\\frac{B}{R+G+B}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "__HSV Colour Space__\n",
    "\n",
    "Convert the RGB image into HSV Colour Space. Use OpenCVâ€™s built-in function *cv2.cvtColor(img, cv2.RGB2HSV)*.\n",
    "\n",
    "__YCbCr Colour Space__\n",
    "\n",
    "Convert the RGB image into YCbCr Colour Space. Use OpenCVâ€™s built-in function *cv2.cvtColor(img, cv2.RGB2YCrCb)*. Note, you need to arrange the channels in $Y, C_b$ and $C_r$ order.\n",
    "\n",
    "__Grayscale__\n",
    "\n",
    "Convert the RGB image into grayscale by using 3 different methods mentioned in\n",
    "https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/\n",
    "In the end, check and report which method OpenCV uses for grayscale conversion, include it as well, and visualize all 4 in the same figure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926d38d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rgb2grays(input_image):\n",
    "    # converts an RGB into grayscale by using 4 different methods\n",
    "    RGB = getColourChannels(input_image)\n",
    "    new_image = []\n",
    "    \n",
    "    # ligtness method\n",
    "    new_image.append((np.maximum.reduce(RGB) + np.minimum.reduce(RGB)) / 2)\n",
    "    # average method\n",
    "    new_image.append(np.mean(RGB, axis=0))\n",
    "    # luminosity method\n",
    "    new_image.append((RGB[0]*0.21 + RGB[1]*0.72 + RGB[2]*0.7) / 3)\n",
    "    # built-in opencv function\n",
    "    new_image.append(cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY))\n",
    "#     print(new_image)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def rgb2opponent(input_image):\n",
    "    # converts an RGB image into opponent colour space\n",
    "    R, G, B = getColourChannels(input_image)\n",
    "    new_image = []\n",
    "    zeros = np.zeros(input_image[:, :, 0].shape)\n",
    "    \n",
    "    # perform opponent converstion but make it so that the values return in the RGB range\n",
    "    O1 = ((R - G) / np.sqrt(2) + (255 / np.sqrt(2))) * 255 * np.sqrt(2) / 510\n",
    "    O2 = (((R + G - 2 * B) / np.sqrt(6)) + (510 / np.sqrt(6))) * 255 * np.sqrt(6) / 1020\n",
    "    O3 = (((R + G + B) / np.sqrt(3)) + (765 / np.sqrt(3))) * 255 * np.sqrt(3) / 1530\n",
    "\n",
    "    new_image.append(np.dstack((O1, O2, O3)))\n",
    "    new_image.append(np.dstack((O1, zeros, zeros)))\n",
    "    new_image.append(np.dstack((zeros, O2, zeros)))\n",
    "    new_image.append(np.dstack((zeros, zeros, O3)))\n",
    "    return new_image\n",
    "\n",
    "def rgb2normedrgb(input_image):\n",
    "    # converts an RGB image into normalized rgb colour space\n",
    "    R, G, B = getColourChannels(input_image)\n",
    "    new_image = []\n",
    "    zeros = np.zeros(input_image[:, :, 0].shape)\n",
    "\n",
    "    r = R / (R + G + B) * 255\n",
    "    g = G / (R + G + B) * 255\n",
    "    b = B / (R + G + B) * 255\n",
    "    \n",
    "    new_image.append(np.dstack((r, g, b)))\n",
    "    new_image.append(np.dstack((r, zeros, zeros)))\n",
    "    new_image.append(np.dstack((zeros, g, zeros)))\n",
    "    new_image.append(np.dstack((zeros, zeros, b)))\n",
    "    return new_image\n",
    "\n",
    "def getColourChannels(input_image):\n",
    "\n",
    "    R = input_image[:, :, 0]\n",
    "    G = input_image[:, :, 1]\n",
    "    B = input_image[:, :, 2]\n",
    "\n",
    "    return [R, G, B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6752f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(input_image, colourspace):\n",
    "    # Fill in this function. Remember to remove the pass command\n",
    "\n",
    "    plot_title = {'rgb': 'Normalized RGB', 'gray': 'Gray', 'ycbcr': 'YCBCR', 'hsv': 'HSV', 'opponent': 'Opponent'}\n",
    "    subplot_titles = {'rgb': ['Converted Image', 'Red', 'Green', 'Blue'],\n",
    "                      'gray': ['Lightness', 'Average', 'Luminance', 'OpenCV'],\n",
    "                      'ycbcr': ['Converted Image', 'Luminance', 'Chromatic Red', 'Chromatic Blue'],\n",
    "                      'hsv': ['Converted Image', 'Hue', 'Saturation', 'Value'],\n",
    "                      'opponent': ['Converted Image', 'O1', 'O2', 'O3']}\n",
    "\n",
    "    fig, axs = plt.subplots(1,4,figsize=(10, 4))\n",
    "\n",
    "    if colourspace == 'gray':\n",
    "        fig.suptitle(f'RGB Image in {plot_title[colourspace]} with four methods')\n",
    "        for i in range(4):\n",
    "            axs[i].imshow(input_image[i].astype(np.uint16), cmap='gray')\n",
    "            axs[i].set_title(subplot_titles[colourspace][i])\n",
    "\n",
    "    else:\n",
    "        fig.suptitle(f'RGB Image in {plot_title[colourspace]} and its channels')\n",
    "        axs[0].imshow(input_image[0].astype(np.uint16))  # plot only first image with color\n",
    "        axs[0].set_title(subplot_titles[colourspace][0])\n",
    "        for i in range(1,4):\n",
    "            axs[i].imshow(input_image[i].astype(np.uint16))\n",
    "            axs[i].set_title(subplot_titles[colourspace][i])\n",
    "\n",
    "    for i in range(0,4):\n",
    "        axs[i].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ConvertColourSpace(input_image, colourspace):\n",
    "    '''\n",
    "    Converts an RGB image into a specified color space, visualizes the\n",
    "    color channels and returns the image in its new color space.\n",
    "\n",
    "    Colorspace options:\n",
    "      opponent\n",
    "      rgb -> for normalized RGB\n",
    "      hsv\n",
    "      ycbcr\n",
    "      gray\n",
    "\n",
    "    P.S: Do not forget the visualization part!\n",
    "    '''\n",
    "\n",
    "    # Convert the image into double precision for conversions\n",
    "    input_image = input_image.astype(np.float32)\n",
    "\n",
    "    if colourspace.lower() == 'opponent':\n",
    "        # fill in the rgb2opponent function\n",
    "        new_image = rgb2opponent(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'rgb':\n",
    "        new_image = rgb2normedrgb(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'hsv':\n",
    "        # use built-in function from opencv\n",
    "        new_image = []\n",
    "        hsv = cv2.cvtColor(input_image, cv2.COLOR_RGB2HSV)\n",
    "        zeros = np.zeros(input_image[:, :, 0].shape)\n",
    "\n",
    "        H = hsv[:, :, 0] / 360 * 255\n",
    "        S = hsv[:, :, 1] * 255\n",
    "        V = hsv[:, :, 2]\n",
    "\n",
    "        new_image.append(np.dstack((H, S, V)))\n",
    "        new_image.append(np.dstack((H, zeros, zeros)))\n",
    "        new_image.append(np.dstack((zeros, S, zeros)))\n",
    "        new_image.append(np.dstack((zeros, zeros, V)))\n",
    "\n",
    "    elif colourspace.lower() == 'ycbcr':\n",
    "        # use built-in function from opencv\n",
    "        new_image = []\n",
    "        ycbcr = cv2.cvtColor(input_image, cv2.COLOR_RGB2YCrCb)\n",
    "        zeros = np.zeros(input_image[:, :, 0].shape)\n",
    "\n",
    "\n",
    "        Y = ycbcr[:, :, 0]\n",
    "        Cb = ycbcr[:, :, 2] + 128\n",
    "        Cr = ycbcr[:, :, 1] + 128\n",
    "\n",
    "        new_image.append(np.dstack((Y, Cb, Cr)))\n",
    "        new_image.append(np.dstack((Y, zeros, zeros)))\n",
    "        new_image.append(np.dstack((zeros, Cb, zeros)))\n",
    "        new_image.append(np.dstack((zeros, zeros, Cr)))\n",
    "\n",
    "    elif colourspace.lower() == 'gray':\n",
    "        new_image = rgb2grays(input_image)\n",
    "\n",
    "    else:\n",
    "        print('Error: Unknown colorspace type [%s]...' % colourspace)\n",
    "        new_image = input_image\n",
    "\n",
    "    visualize(new_image, colourspace)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    ## Replace the image name with a valid image\n",
    "    #img_path = 'test.png'\n",
    "    ## Read with opencv\n",
    "    #I = cv2.imread(img_path)\n",
    "    ## Convert from BGR to RGB\n",
    "    ## This is a shorthand.\n",
    "    #I = I[:, :, ::-1]\n",
    "\n",
    "    #out_img = ConvertColourSpace(I, 'opponent.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace the image name with a valid image\n",
    "img_path = 'images/awb/awb.jpg'\n",
    "# Read with opencv\n",
    "I = cv2.imread(img_path)\n",
    "# Convert from BGR to RGB\n",
    "# This is a shorthand.\n",
    "I = I[:, :, ::-1]\n",
    "\n",
    "out_img = ConvertColourSpace(I, 'rgb')\n",
    "out_img = ConvertColourSpace(I, 'opponent')\n",
    "out_img = ConvertColourSpace(I, 'hsv')\n",
    "out_img = ConvertColourSpace(I, 'ycbcr')\n",
    "out_img = ConvertColourSpace(I, 'gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.1**\n",
    "We use the RGB color model because it comprises the three primary additive colors, red, green and blue. With these three colors we can produce any color we want, including white, which occurs when all colors are mixed [Szeliski Book Fig. 2.8]. We use additive colors because the contrast they create with black screens works very well to produce digital images [1]. RGB is also convenient because it mimics the trichromatic nature of the human eye, which has different types of receptors called cones to sense red, green and blue [Lecture 2]. To capture a colored image, a standard digital camera is equipped with sensors that translate the wavelength of the photons to an RGB value. To achieve this, the sensors contain a Bayer Filter, which has the appearance of a checkboard with red, green and blue squares [Lecture 2].\n",
    "\n",
    "[1] https://medium.com/nerd-for-tech/the-color-theory-why-do-computers-use-rgb-instead-of-ryb-205b75d6e783\n",
    "\n",
    "**2.2**\n",
    "cv2.COLOR_BGR2GRAY uses the luminosity method (run the above cell with 'out_img = ConvertColourSpace(I, 'gray')', the differences between the images can be seen clearly in the top left of each image). "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Colour Space Properties (5pts)\n",
    "\n",
    "Explain each of those 5 colour spaces and their properties. What are the benefits of using a different colour space other than RGB? Provide reasons for each of the above cases. You can include your observations from the visualizations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The opponent color space adheres to the opponent process color theory of Ewald Hering, which states that certain colors oppose one another as they are measured (green diminishes red for example). The main benefits of opponent color space is that is computationally cheap to implement (a linear transformation on RGB) and more closely resembles how humans see color (according to proponents of opponent process color theory at least) [1]. Additionally, the color is lightning invariant (changing the lighting transforms red + green + blue but the difference between red and green remains the same).\n",
    "\n",
    "* A normalized RGB color image will have its RGB values proportional to one another (r + b + g will always equal 1). This effectively removes lighting effects on images and can thus be usefull for object tracking such that the color of a moving object will stay consistent even as it moves into different lighting conditions.\n",
    "\n",
    "* The HSV color space has three components: Hue (the color represented as 0 to 360 degrees angle), Saturation (the greyness scale), Value (the brightness scale). The main benefit of HSV is that it more closely represents how humans see color (as opposed to RGB) and is thus more intuitive to use. Like with opponent color space, lighting is seperated from color. \n",
    "\n",
    "* The YCbCr color space three components: Y (lumonisity), Cb (difference of blue), Cr (difference of red). Its main application is in compression [Szeliski Book 2.3.3 page 98], since it contains less redundancy than RGB and is thus more efficient. For example, Y is often compressed in higher fidelity than Cb and Cr since the human eye is more sensitive to changes in lighting than to changes in color. \n",
    "\n",
    "* A grayscale image focuses soley on intensity and will be computationally efficient for applications that do not bother with chromacity.\n",
    "\n",
    "[1] https://graphics.stanford.edu/~boulos/papers/orgb_sig.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 More on Colour Spaces (2pts)\n",
    "\n",
    "Find one more colour space from the literature and simply explain its properties and give a use case."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CIELAB "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Intrinsic Image Decomposition (15pts)\n",
    "Intrinsic image decomposition is the process of separating an image into its formation components, such as reflectance (albedo) and shading (illumination). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Then, under the assumption of body (diffuse) reflection, linear sensor response and narrow band filters, the decomposition of the observed image $I(\\vec{x})$ at position $\\vec{x}$ can be approximated as the element-wise product of its albedo $R(\\vec{x})$ and shading $S(\\vec{x})$ intrinsics:\n",
    "\n",
    "$$I(\\vec{x})=R(\\vec{x}) \\times S(\\vec{x})$$\n",
    "\n",
    "In this part of the assignment, you will experiment with intrinsic image components to perform one of the computational photography applications; material recolouring. For the experiments, we will use images from a synthetic intrinsic image dataset. <a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2)\n",
    "\n",
    "<a name=\"cite_note-1\"></a><small>1. [^](#cite_ref-1) H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3-26, 1978.</small>\n",
    "\n",
    "<a name=\"cite_note-2\"></a><small>2. [^](#cite_ref-1) http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. What other components can an image be decomposed other than albedo and shading Give an example and explain your reasoning. *(4pts)*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other components of an image can be shape and pose [Lecture 3 p.42]. For instance, in 3D reconstruction from images of different objects, to be able to recreate the object from its components, albedo and illumination is not sufficient. We also need information about the shape (vector field of surface normal of object) and pose (how the object is placed in the space) to synthesize the object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. If you check the literature, you will see that almost all the intrinsic image decomposition datasets are composed of synthetic images. What might be the reason for that? *(2pts)*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because it is very easy to model light in a 3D environment with artificial objects and capture millions of images automatically. Also, it is possible to extract the ground-truth intrinsic components of the object through the computer modeling software. On the other hand, capturing those images in different lighting conditions in real life and extracting the ground-truth intrinsic components is a very tedious and time-consuming manual process [Intrinsic Images in the Wild, Sean Bell et al.]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Show that you can actually reconstruct the original *turtle.png* image from its intrinsics using *turtle_albedo.png* and *turtle_shading.png*. In the end, your script should output a figure displaying the original image, its intrinsic images and the reconstructed one. Complete the code for function **iid_image_formation()**. *(4pts)*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def iid_image_formation(albedo_img, shading_img):\n",
    "    img = albedo_img * shading_img\n",
    "    return img\n",
    "\n",
    "img = plt.imread(os.path.join(os.getcwd(),'images','intrinsic_images','turtle.png'))\n",
    "albedo_img = plt.imread(os.path.join(os.getcwd(),'images','intrinsic_images','turtle_albedo.png'))\n",
    "shading_img = plt.imread(os.path.join(os.getcwd(),'images','intrinsic_images','turtle_shading.png'))\n",
    "prod_img = iid_image_formation(albedo_img,shading_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(20,3))\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(albedo_img)\n",
    "axs[1].set_title('Albedo image')\n",
    "axs[2].imshow(shading_img)\n",
    "axs[2].set_title('Shading image')\n",
    "axs[3].imshow(prod_img)\n",
    "axs[3].set_title('Reconstructed image')\n",
    "fig.suptitle('Fig 3.1: Image of a turtle and its components')\n",
    "for i in range(4):\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recoloring *(5pts)*\n",
    "Manipulating colours in photographs is an important problem with many applications in computer vision. Since the aim for recolouring algorithms is just to manipulate colours, better results can be obtained for such a task if the albedo image is available as it is independent of confounding illumination effects.\n",
    "\n",
    "Assume that you are given the *turtle.png* image and you have access to its\n",
    "intrinsic images *turtle_albedo.png* and *turtle_shading.png*.\n",
    "1. Find out the true material colour of the turtle in RGB space (which is uniform in this case).\n",
    "2. Recolour the turtle image with pure green (0, 255, 0). Display the original turtle image and the recoloured version on the same figure. Complete the code for function **recoloring()**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recoloring(albedo_img, shading_img):\n",
    "    recolored_albedo_img = albedo_img.copy()\n",
    "    old_color = albedo_img[int(albedo_img.shape[0]/2),int(albedo_img.shape[1]/2)]  # Get color of center pixel (non-black)\n",
    "    for i in range(albedo_img.shape[0]):\n",
    "        for j in range(albedo_img.shape[1]):\n",
    "            if all(albedo_img[i,j] != [0,0,0]):  # If pixel is non-black\n",
    "                recolored_albedo_img[i,j] = [0,1,0]  # Recolor pixel as pure green\n",
    "                recolored_img = iid_image_formation(recolored_albedo_img, shading_img)  # Get recolored image\n",
    "    return old_color, recolored_img\n",
    "\n",
    "old_color, recolored_img, = recoloring(albedo_img, shading_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(recolored_img)\n",
    "axs[1].set_title('Recolored image')\n",
    "\n",
    "fig.suptitle(f'Fig 3.2: Image of a turtle and its recoloring from {tuple([int(i) for i in 255*old_color])} to (0,255,0)')\n",
    "for i in range(2):\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Although you have recoloured the object with pure green, the reconstructed images do not seem to display those pure colors and thus the colour distributions over the object do not appear uniform. Explain the reason."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This happens because of the multiplication of the albedo matrix with the shading matrix. The shading matrix has non-1 values, which means that, when multiplying it with the pure green color of the albedo, some pixels will have green tones of different lightness depending on the value of the shading."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this was a simple case where the image is synthetic, object centered and has only one colour, and you have access to its ground-truth intrinsic images. Real world scenarios require more than just replacing a single colour with another, not to mention the complexity of achieving a decent intrinsic image decomposition."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Colour Constancy (15pts)\n",
    "\n",
    "Colour constancy is the ability to perceive colors of objects, invariant to the colour of the light source. The aim for colour constancy algorithms is first to estimate the illuminant of the light source, and then correct the image so that the corrected image appears to be taken under a canonical (white) light source. The task of the automatic white balance (AWB) is to do the same in digital cameras so that the images taken by a digital camera look as natural as possible.\n",
    "\n",
    "In this part of the assignment, you will implement the most famous colour constancy algorithm; *Grey-World Algorithm*.\n",
    "\n",
    "### Grey-World Algorithm\n",
    "The algorithm assumes that, under a white light source, the average colour in a scene should be achromatic (grey, [128, 128, 128]).\n",
    "\n",
    "1. Complete the function to apply colour correction to an RGB image by using Grey-World algorithm. Display the original image and the colour corrected one on the same figure. Use awb.jpg image to test your algorithm. In the end, you should see that the reddish colour cast on the image is removed and it looks more natural.\n",
    "\n",
    "  ***Note:*** You do not need to apply any pre or post processing steps. For the calculation or processing, you are not allowed to use any available code or any dedicated library function except *standard Numpy functions*.\n",
    "\n",
    "   ***Hint:*** Check the von Kries model for this step.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grey_world(awb_img):\n",
    "    \"\"\"\n",
    "    ================\n",
    "    Your code here\n",
    "    ================\n",
    "    \"\"\"\n",
    "    \n",
    "    R = awb_img[:, :, 0]\n",
    "    G = awb_img[:, :, 1]\n",
    "    B = awb_img[:, :, 2]\n",
    "\n",
    "    Alpha_R = ((R / np.mean(R.flatten()) * 128) )\n",
    "    Beta_G = ((G / np.mean(G.flatten()) * 128) )\n",
    "    Gamma_B = ((B / np.mean(B.flatten()) * 128) )\n",
    "    \n",
    "#     Alpha_R *= 255 / np.max(Alpha_R)\n",
    "#     Beta_G *= 255 / np.max(Beta_G)\n",
    "#     Gamma_B *= 255 / np.max(Gamma_B)\n",
    "    \n",
    "    return np.dstack((Alpha_R.astype(np.uint16), Beta_G.astype(np.uint16), Gamma_B.astype(np.uint16)))\n",
    "\n",
    "img_path = 'images/awb/awb.jpg'\n",
    "I = cv2.imread(img_path)\n",
    "I = I[:, :, ::-1]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(I)\n",
    "axs[1].set_title('Color Corrected Image')\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(grey_world(I))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Give an example case for Grey-World Algorithm on where it might fail. Remember to include your reasoning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Grey-World Algorithm might fail on an image that contains large regions of one dominant color. In this case it can not distinguish between the inherent color of the object and color superimposed due to lighting conditions because it assumes the average colour is anchromatic."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Find out one more colour constancy algorithms from the literature and explain it briefly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "93ac18c3",
   "metadata": {},
   "source": [
    "3. Find out one more colour constancy algorithms from the literature and explain it briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf9fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}