{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7PRKfXw99hy"
   },
   "source": [
    "# [*Lab Project Part 1*]() Image Classification using Bag-of-Words\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY6wdmc299h1"
   },
   "source": [
    "# **General Guideline**\n",
    "1. Aim:\n",
    "    - Able to understand the basic Image Classification/Recognition pipeline using traditional Bag of Words method.\n",
    "    - Able to use to python packages for image classification: *matplotlib, cv2, sklearn etc.*\n",
    "2. Prerequisite:\n",
    "    - Familiar with python and relevant packages.\n",
    "    - Know the basics of feature descriptors(SIFT, HoG) and machine learning tools (K-means, SVM and etc.). \n",
    "3. Guidelines:\n",
    "    Students should work on the assignments in a group of **three person** for two weeks. Some minor additions and changes  might happen with approval from the Senior TA. Students will be informed for these changes via Canvas. Any questions regarding the assignment content can be discussed on Piazza. Students are expected to do this assignment in Python and Pytorch, however students are free to choose other tools (like Tensorflow). Your source code and report must be handed in together in a zip file (*ID1_ID2_ID3.zip*) before the deadline. Make sure your report follows these guidelines:\n",
    "    - *The maximum number of pages is 10 (single-column, including tables and figures). Please express your thoughts concisely.*\n",
    "    - *Follow the given script and answer all given questions (in green boxes). Briefly describe what you implemented. Blue boxes are there to give you hints to answer questions.*\n",
    "    - *Analyze your results and discuss them, e.g. why algorithm A works better than algorithm B on a certain problem.*\n",
    "    - *Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.*\n",
    "4. **Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs' system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "5. **Plagiarism note**: \n",
    "Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations.\n",
    "\n",
    "<!-- ### PyTorch versions\n",
    "we assume that you are using latest PyTorch version(>=1.4)\n",
    "\n",
    "### PyTorch Tutorial & Docs\n",
    "This tutorial aims to make you familiar with the programming environment that will be used throughout the course. If you have experience with PyTorch or other frameworks (TensorFlow, MXNet *etc.*), you can skip the tutorial exercises; otherwise, we suggest that you complete them all, as they are helpful for getting hands-on experience.\n",
    "\n",
    "**Anaconda Environment** We recommend installing \\textit{anaconda} for configuring \\textit{python} package dependencies, whereas it's also fine to use other environment managers as you like. The installation of anaconda can be found in [anaconda link](https://docs.anaconda.com/anaconda/install/).\n",
    "\n",
    "**Installation** The installation of PyTorch is available at [install link](https://pytorch.org/get-started/locally/) depending on your device and system.\n",
    "\n",
    "**Getting start** The 60-minute blitz can be found at [blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), and and examples are at [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "**Documents** There might be potential unknown functions or classes, you shall look through the official documents website ([Docs](https://pytorch.org/docs/stable/index.html)) and figure them out by yourself. (***Think***:} What's the difference between *torch.nn.Conv2d* and *torch.nn.functional.conv2d*?)\n",
    "You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). In this assignments, we wish you to form the basic capability of using one of the well-known   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2EILNe3JR1b"
   },
   "source": [
    "# **Instruction**\n",
    "\n",
    "1. Students are expected to prepare a report for this project. The report should include the analysis of the results for different settings.\n",
    "\n",
    " Do not just provide numbers, remember to follow the general guidelines and discuss different settings.\n",
    "\n",
    "2. For qualitative evaluation, you are expected to visualize the top-5 and the bottom-5 ranked test images (based on the classifier confidence for the target class) per setup. That means you are supposed to provide a figure for each experimental setup, as discussed in Section 2.6.\n",
    "\n",
    "3. A demo function which runs the whole system should be prepared and submitted with all other implemented functions.\n",
    "\n",
    "**Hint:** Having visual elements such as charts, graphs and plots are always useful for everyone. Keep this in mind while writing your reports. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uNsGM0F9rfE"
   },
   "source": [
    "# **1. Introduction**\n",
    "\n",
    "The goal of the assignment is to implement a system for image classification. In other words, this system should tell if there is an object of given class in an image. You will perform 5-class ({1: *airplanes*, 2: *birds*, 3: *ships*, 4: *horses*, 5: *cars*}) image classification based on bag-of-words approach ([reference](http://www.robots.ox.ac.uk/~az/icvss08_az_bow.pdf)) using SIFT features, respectively. [STL-10 dataset](https://cs.stanford.edu/~acoates/stl10/) will be used for the task. For each class, test sub-directories contain 800 images, and training sub-directories contain 500 images. Images are represented as (RGB) 96x96 pixels.\n",
    "\n",
    "Download the [dataset](http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz). There are five files: *test_X.bin*, *test_y.bin*, *train_X.bin*,*train_y.bin* and *unlabeled_X.bin*. For the project, you will just use the train and test partitions. Download the dataset and make yourself familiar with it by figuring out which images and labels you need for the aforementioned 5 classes. Note that you do not need *fold_indices* variable.\n",
    "\n",
    "**Hint:**\n",
    "In a real scenario, the public data you use often deviates from your task. You need to figure it out and re-arrange the labels as required using *stl10\\_input.py* as a reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJZPyMT7_Kvz"
   },
   "source": [
    "## **1.1 Training Phase**\n",
    "\n",
    "Training must be conducted over the training set. Keep in mind that using more samples in training will likely result in better performance. However, if your computational resources are limited and/or your system is slow, it's OK to use less number of training data to save time.\n",
    "\n",
    "**Hint:** To debug your code, you can use a small amount of input images/descriptors. Once you are sure everything works properly, you can run your code for the experiment using all the data points. \n",
    "\n",
    "**Hint:** You are not allowed to use the test images for training purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDzqpqAn_ueU"
   },
   "source": [
    "## **1.2 Training Phase**\n",
    "\n",
    "You have to test your system using the specified subset of test images. All 800 test images should be used at once for testing to observe the full performance. Again, exclude them from training for fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma47S1_-_-aN"
   },
   "source": [
    "# **2. Bag-of-Words based Image Classification**\n",
    "\n",
    "Bag-of-Words based Image Classification system contains the following steps: \n",
    "1. Feature extraction and description\n",
    "2. Building a visual vocabulary\n",
    "3. Quantify features using visual dictionary (encoding)\n",
    "4. Representing images by frequencies of visual words\n",
    "5. Train the classifier\n",
    "\n",
    "We will consider each step in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXQBnhT499h2"
   },
   "source": [
    "## **2.1 Feature Extraction and Description**\n",
    "\n",
    "SIFT descriptors can be extracted from either (1) densely sampled regions or (2) key points. You can use SIFT related functions in *OpenCV* for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvNbVoMh99h5"
   },
   "source": [
    "####  **` Q2.1: Extract SIFT descriptor from training datasets based on both densely sampled regions and key points. For both extraction approaches, show two image from each of the five class (draw the circles with size of keypoint). (10-pts).`**  \n",
    "\n",
    "**Hint:**\n",
    "Check out the Docs of SIFT and related functions for further information in the following [link1](https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html) and [link2](https://docs.opencv.org/3.4.9/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html).\n",
    "\n",
    "**Note:**\n",
    "For copyright reason, the newest version of OpenCV does not contain SIFT related function. However you can install an old version (for example: opencv-python==3.4.2.17 and opencv-contrib-python==3.4.2.17). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47K1Kzph99h3"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# referenced codes: \n",
    "######################################################\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from stl10_input import *\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# labels = read_labels('stl10_binary\\class_names.txt')\n",
    "allimages = read_all_images('stl10_binary/test_X.bin')\n",
    "alllabels = read_labels('stl10_binary/test_y.bin')\n",
    "relevant_classes = np.array([1, 2, 9, 7, 3])\n",
    "\n",
    "images, labels = keep_relevant_images(allimages, alllabels, relevant_classes)\n",
    "\n",
    "airplane_images = keep_relevant_images(images, labels, 1)[0]\n",
    "bird_images = keep_relevant_images(images, labels, np.array([2]))[0]\n",
    "car_images = keep_relevant_images(images, labels, np.array([3]))[0]\n",
    "horse_images = keep_relevant_images(images, labels, np.array([7]))[0]\n",
    "ship_images = keep_relevant_images(images, labels, np.array([9]))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keypoints_DSR(img):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    # image is 96x96 so take points with stride 8\n",
    "    keypoints = [cv2.KeyPoint(x, y, 8) for y in range(8, 96, 8) for x in range(8, 96, 8)]\n",
    "    descriptors = sift.compute(img, keypoints)\n",
    "#     kpimage=cv2.drawKeypoints(img,keypoints, None, flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS) \n",
    "#     plot_image(kpimage)    \n",
    "    return keypoints, descriptors\n",
    "            \n",
    "def Keypoints_SIFT(img):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "#     kpimage=cv2.drawKeypoints(img,keypoints, None, flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "#     plot_image(kpimage)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def Keypoints_DEMO():\n",
    "    DEMO_images = [*airplane_images[:2], *bird_images[:2], *car_images[:2], *horse_images[:2], *ship_images[:2]]\n",
    "    for img in DEMO_images:\n",
    "        DSR_kp, DSR_des = Keypoints_DSR(img)\n",
    "        SIFT_kp, SIFT_des = Keypoints_SIFT(img)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n",
    "        fig1 = ax1.imshow(cv2.drawKeypoints(img,DSR_kp, None, flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS))\n",
    "        fig2 = ax2.imshow(cv2.drawKeypoints(img,SIFT_kp, None, flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS))\n",
    "        ax1.set_title('Densely Sampled Regions Method')\n",
    "        ax2.set_title('SIFT Method')\n",
    "        plt.show()\n",
    "\n",
    "Keypoints_DEMO()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ6OkCV9LIM9"
   },
   "source": [
    "## **2.2 Building Visual Vocabulary**\n",
    "\n",
    "Here, we will obtain visual words by clustering feature descriptors, so each cluster center is a visual word. Take a subset (maximum half) of all training images (this subset should contain images from ALL categories), extract SIFT descriptors from all of these images, and run k-means clustering (you can use your favourite k-means implementation) on these SIFT descriptors to build visual vocabulary. Then, take the rest of the training images to calculate visual dictionary. Nonetheless, you can also use less images, say 100 from each class (exclusive from the previous subset) if your computational resources are limited. Pre-defined cluster numbers will be the size of your vocabulary. In this question, set its size to 1000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6a1z2vK99h7"
   },
   "source": [
    "####  **` Q2.2: Building Visual Vocabulary. (10-pts)`**\n",
    "Create visual vocabulary by using K-means clustering. Remember to display the results when the vocabulary subset is 30\\%, 40\\%, 50\\% and 60\\% amount of the training images. The vocabulary size is fixed 1000 in this question.\n",
    "\n",
    "**Hint:** Remember first to debug all the code with a small amount of input images and only when you are sure that code functions correctly run it for training over the larger data. You can achieve K-means clustering using either \\textit{sklearn} package or \\textit{scipy} package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu1004H599h9"
   },
   "outputs": [],
   "source": [
    "###################### \n",
    "# code\n",
    "######################\n",
    "K = 1000\n",
    "subset = [*airplane_images[:100], *bird_images[:100], *car_images[:100], *horse_images[:100], *ship_images[:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gihEWG9MC8b8"
   },
   "source": [
    "## **2.3 Encoding Features Using Visual Vocabulary**\n",
    "\n",
    "Once we have a visual vocabulary, we can represent each image as a collection of visual words. For this purpose, we need to extract feature descriptors (SIFT) and then assign each descriptor to the closest visual word from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9GRPT4kDGRt"
   },
   "source": [
    "## **2.4 Representing images by frequencies of visual words**\n",
    "\n",
    "The next step is the quantization. The idea is to represent each image by a histogram of its visual words. Check out ***matplotlib***'s *hist* function. Since different images can have different numbers of features, histograms should be normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brdvDtpTDeoA"
   },
   "source": [
    "####  **` Q2.4: Representing images by frequencies of visual words. (5-pts)`**\n",
    "\n",
    "Pick one of the subset ratios from the above four settings (30%, 40%, 50% and 60%). Show the histogram of each class\n",
    "under this setting. Describe the similarities and differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ER-moXF99h9"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzvBkirXDubr"
   },
   "source": [
    "## **2.5 Classification**\n",
    "\n",
    "We will train a classifier per each object class. Now, we take the Support Vector Machine (SVM) as an example. As a result, we will have 5 binary classifiers. Take images from the training set of the related class (should be the ones which you did not use for dictionary calculation). Represent them with histograms of visual words as discussed in the previous section. Use at least 50 training images per class or more, but remember to debug your code first! If you use the default setting, you should have 50 histograms of size 500. These will be your positive examples. Then, you will obtain histograms of visual words for images from other classes, again about 50 images per class, as negative examples. Therefore, you will have 200 negative examples. Now, you are ready to train a classifier. You should repeat it for each class. To classify a new image, you should calculate its visual words histogram as described in Section 2.4 and use the trained SVM classifier to assign it to the most probable object class. (Note that for proper SVM scores you need to use cross-validation to get a proper estimate of the SVM parameters. In this assignment, you do not have to experiment with this cross-validation step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaZtBMX9D7UR"
   },
   "source": [
    "####  **` Q2.5: Classification (5-pts)`**\n",
    "\n",
    "Utilize SVM and finish classification training.\n",
    "\n",
    "**Hint:**\n",
    "You can use *scikit-learn* software to conduct SVM classification. The relevant documents can be found at [link](https://scikit-learn.org/stable/modules/svm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvuuuJBt99h-"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT2FNr3rET3l"
   },
   "source": [
    "## **2.6 Evaluation**\n",
    "\n",
    "To evaluate your system, you should take all the test images from all classes and rank them based on each binary classifier. In other words, you should classify each test image with each classifier and then sort them based on the classification score. As a result, you will have five lists of test images. Ideally, you would have images with airplanes on the top of your list which is created based on your airplane classifier, and images with cars on the top of your list which is created based on your car classifier, and so on.\n",
    "\n",
    "In addition to the qualitative analysis, you should measure the performance of the system quantitatively with the Mean Average Precision over all classes. The Average Precision for a single class c is defines as\n",
    "\\begin{equation}\n",
    "\\frac{1}{m_c} \\sum_{i=1}^{n} \\frac{f_c(x_i)}{i}\\mbox{ ,}\n",
    "\\end{equation}\n",
    "where $n$ is the number of images ($n=50\\times 5=250$), $m$ is the number of images of class $c$ ($m_c=50$), $x_i$ is the $i^{th}$ image in the ranked list $X = \\left \\{ x_1, x_2, \\dots, x_n  \\right \\}$, and finally, $f_c$ is a function which returns the number of images of class $c$ in the first $i$ images if $x_i$ is of class $c$, and 0 otherwise. To illustrate, if we want to retrieve $R$ and we get the following sequence: $[R, R, T, R, T, T, R, T]$, then $n = 8$, $m = 4$, and $AP(R, R, T, R, T, T, R) = \\frac{1}{4} \\left (  \\frac{1}{1} + \\frac{2}{2} + \\frac{0}{3} + \\frac{3}{4} + \\frac{0}{5} + \\frac{0}{6} + \\frac{4}{7} + \\frac{0}{8} \\right )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cjfUXneEubT"
   },
   "source": [
    "####  **` Q2.6: Evaluation and Discussion (30-pts)`**\n",
    "\n",
    "Show the evaluation results and describe. For the qualitative evaluation, you are expected to visualize the top-5 and the bottom-5 ranked test images (based on the classifier confidence for the target class) per setup. The report should include the analysis of the results for different settings such as:\n",
    "- mAP based on different subset ratios to create the vocabulary list (30%, 40%, 50% and 60%) under the fixed vocabulary size 1000.\n",
    "- Based on the ratio among the above four settings that lead to the best performance, change the vocabulary sizes to different sizes (500, 1000, 1500, 2000). Report and discuss the mAP.\n",
    "- Based on the above experiments, find the best setting. Report the mAP based on SIFT descriptor and HoG descriptor. \n",
    "- The impact of the hyper-parameters of SVM.  \n",
    "\n",
    "**Hint 1:**\n",
    "To alleviate the working load, the discussion on the impact of SVM’s hyper-parameter settings only need to based on the optimal settings from the first three questions.\n",
    "\n",
    "**Hint 2:**\n",
    "Be sure to discuss the differences between different settings such as vocabulary sizes in your report.\n",
    "\n",
    "**Hint 3:**\n",
    "You can use *skimage.feature.hog* to extract HoG descriptor. The relevant documents can be found at [link](https://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=hog#skimage.feature.hog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeCMyWD3FSIB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
