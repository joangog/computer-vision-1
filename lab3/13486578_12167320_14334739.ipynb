{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Assignment 3 Harris Corner Detector & Optical Flow </center>\n",
    "<center> Computer Vision 1 University of Amsterdam </center>\n",
    "    <center> Due 23:59, October 1, 2022 (Amsterdam time) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Guidelines\n",
    "Your code and discussion must be completed in this **jupyter notebook** before the deadline by submitting it to the Canvas Lab 3 Assignment. Submit your assignment in a **zip file**, with all the relevant files and images need to run your notebook. Name your zip file as follows:  **StudentID1_StudentID2_StudentID3.zip**\n",
    "For full credit, make sure your answers follow these guidelines:\n",
    "\n",
    "- Make sure you use the provided python environment. You can create the environment using conda and the provided YAML file using the following command: `conda env create --file=CV1_env.yaml`, then activate it as `conda activate cv1`. Using different packages versions may result in the impossibility to run the submitted code and therefore in the subtraction of points. Below you will find a code cell to check the versions of your packages. \n",
    "- Please express your thoughts concisely. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions (in **<font color=green >Question** parts). Briefly describe what you implemented. **<font color=blue >Hint** parts are there to give you hints to answer questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidence (qualitative and/or quantitative results, references to papers, etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make your notebook self-contained and complete.\n",
    "- Analyze your results and discuss them, e.g. why algorithm A works better than algorithm B in a certain problem.\n",
    "- Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "- Make sure all the code in your notebook runs without errors or bugs before submitting. Code that does not run can result in a lower grade. \n",
    "\n",
    "**Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAsâ€™ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "\n",
    "**Plagiarism note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages needed for this assignment\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import convolve2d as conv\n",
    "from scipy.ndimage import maximum_filter, gaussian_filter, rotate\n",
    "import os\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"3.4.2\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.19.5\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.3.4\", \"You're not using the provided Python environment!\"\n",
    "assert scipy.__version__ == \"1.7.3\", \"You're not using the provided Python environment!\"\n",
    "# Proceed to the next cell if you don't get any error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Harris Corner Detector (45pts)\n",
    "In this section, a derivation of the *[Harris Corner Detector](https://www.semanticscholar.org/paper/A-Combined-Corner-and-Edge-Detector-Harris-Stephens/6818668fb895d95861a2eb9673ddc3a41e27b3b3)* is presented.\n",
    "\n",
    "Given a shift $(\\Delta x,\\Delta y)$ at a point $(x, y)$, the auto-correlation function is defined as:\n",
    "\n",
    "$$c(\\Delta x,\\Delta y) = \\sum\\limits_{(x,y)\\in W(x,y)} {w(x,y)(I(x+\\Delta x,y+\\Delta y)-I(x,y))^2} \\tag {1}$$\n",
    "\n",
    "where $W(x,y)$ is a window centered at point $(x,y)$ and $w(x,y)$ is a Gaussian function. For simplicity, from now on, $\\sum\\limits_{(x,y)\\in W(x,y)}$ will be referred to as $\\sum\\limits_{W}$.\n",
    "Approximating the shifted function by the first-order Taylor expansion we get:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "I(x+\\Delta x,y+ \\Delta y) & \\approx & I(x,y) + I_x(x,y)\\Delta x + I_y(x,y)\\Delta y \\tag {2} \\\\\n",
    "&=& I(x,y) + [I_x(x,y) \\ I_y(x,y)] \\begin{bmatrix} \\Delta x \\\\ \\Delta y \\end{bmatrix}, \\tag {3}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $I_x$ and $I_y$ are partial derivatives of $I(x,y)$. The first gradients can be approximated by: \n",
    "\\begin{eqnarray}\n",
    "    I_x &=& \\frac{\\partial I}{\\partial x} \\approx I * G_x, \\quad G_x = (-1,0,1) \\tag {4} \\\\ \n",
    "    I_y &=& \\frac{\\partial I}{\\partial y} \\approx I * G_y, \\quad G_y = (-1,0,1)^T \\tag {5}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Note that using the kernel $(-1, 1)$ to approximate the gradients is also correct. The auto-correlation function can now be written as:\n",
    "\\begin{eqnarray}\n",
    "c(\\Delta x,\\Delta y) &=& \\sum\\limits_{W} {w(x,y) (I(x+\\Delta x,y+\\Delta y)-I(x,y))^2} \\tag {6} \\\\\n",
    "& \\approx & \\sum\\limits_{W} {w(x,y) ([I_x(x,y) \\ I_y(x,y)]\\begin{bmatrix} \\Delta x \\\\ \n",
    "\\Delta y \\end{bmatrix})^2} \\tag {7} \\\\\n",
    "&=& [\\Delta x \\ \\Delta y]Q(x,y)\\begin{bmatrix} \\Delta x \\\\ \\Delta y \\end{bmatrix}, \\, \\tag {8}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $Q(x,y)$ is given by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Q(x,y) &=& \\sum\\limits_{W} w(x,y){\\begin{bmatrix} I_x(x,y)^2  & I_x(x,y)I_y(x,y)  \\\\  I_x(x,y)I_y(x,y)  & I_y(x,y)^2   \\end{bmatrix}} \\tag {9} \\\\\n",
    "&=& \\begin{bmatrix} \n",
    "\\sum\\limits_{W}{I_x(x,y)^2} * w(x, y) & \\sum\\limits_{W}{I_x(x,y)I_y(x,y)} * w(x, y) \\\\  \n",
    "\\sum\\limits_{W}{I_x(x,y)I_y(x,y)} * w(x, y) & \\sum\\limits_{W}{I_y(x,y)^2} * w(x, y)  \\end{bmatrix} \\tag {10}\\\\\n",
    "&=& \\begin{bmatrix} A & B \\\\  B & C  \\end{bmatrix}.\\, \\tag {11}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *cornerness* $H(x,y)$ is defined by the two eigenvalues of $Q(x,y)$, e.g. $\\lambda_1$ and $\\lambda_2$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\label{eq12}\n",
    "H &=& \\lambda_1 \\lambda_2 - 0.04(\\lambda_1 + \\lambda_2)^2 \\tag{12}\\\\\n",
    "&=& det(Q) - 0.04(trace(Q))^2 \\tag{13}\\\\\n",
    "&=& (AC - B^2) - 0.04(A+C)^2. \\ \\tag{14}\n",
    "\\end{eqnarray}\n",
    "\n",
    "In this section, you are going to implement $Equation ~\\eqref{eq12}$ to calculate $H$ and use it to detect the corners in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue > Hint**: For that purpose, you need to compute the elements of **Q**, i.e. $A$, $B$ and $C$. To do that, you need to calculate $I_x$, which is the smoothed derivative of the image. That can be obtained by convolving the first order Gaussian derivative, $G_d$, with the image $I$ along the x-direction. Then, **A** can be obtained by squaring $I_x$, and then convolving it with a Gaussian, $G$. Similarly, **B** and **C** can be obtained. For example, to get **C**, you need to convolve the image with $G_d$ along the y-direction (to obtain $I_y$), raise it to the square, then convolve it with $G$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue > Hint**: The corner points are the local maxima of **H**. Therefore, you should check for every point in $H$, (1) if it is greater than all its neighbours (in an $n \\times n$ window centered around this point) and (2) if it is greater than the user-defined threshold. If both conditions are met, then the point is labeled as a corner point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green > Question - 1 (35-*pts*)**\n",
    "    \n",
    "1. Create a function to implement the Harris Corner Detector. Your function should return matrix $H$, the indices of rows of the detected corner points **r**, and the indices of columns of those points **c**, where the first corner is given by $(r[0],c[0])$.\n",
    "\n",
    "    \n",
    "2. Implement another function that plots three figures: The computed image derivatives $I_x$ and $I_y$, and the original image with the corner points plotted on it. Show your results on example images **toy/00000001.jpg** and **basketball/085.jpg**. Remember to experiment with different threshold values to see the impact on which corners are found.\n",
    "    \n",
    "    \n",
    "3. Is the algorithm rotation-invariant? How about your implementation? Rotate **toy/00000001.jpg** image 45 and 90 degrees and run the Harris Corner Detector algorithm on the rotated images. Explain your answer and support it with your observations.\n",
    "\n",
    "*Note:* You are allowed to use *scipy.signal.convolve2d* to perform convolution, and *scipy.ndimage.gaussian\\_filter* to obtain your image derivatives. \n",
    "Include a demo function to run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1\n",
    "\n",
    "\n",
    "def get_I_x(I):  # Partial derivative of intensity I by x\n",
    "    G_x = np.array([-1, 0, 1]).reshape((1,3))\n",
    "    I_x = conv(I, G_x, mode='same')\n",
    "    return I_x\n",
    "\n",
    "\n",
    "def get_I_y(I):  # Partial derivative of intensity I by y\n",
    "    G_y = np.array([-1, 0, 1]).reshape((1,3)).T\n",
    "    I_y = conv(I, G_y, mode='same')\n",
    "    return I_y\n",
    "          \n",
    "\n",
    "def get_H(I, W_size, sigma):  # Cornerness function\n",
    "    \n",
    "    I_x = get_I_x(I)\n",
    "    I_y = get_I_y(I)\n",
    "    \n",
    "    A = gaussian_filter(I_x**2, sigma)\n",
    "    B = gaussian_filter(I_x * I_y, sigma)\n",
    "    C = gaussian_filter(I_y**2, sigma)\n",
    "\n",
    "    H = (A*C - B**2) - 0.04*(A + C)**2\n",
    "    H = (H - np.min(H)) / np.max(H - np.min(H))  # Normalize H so that the threshold selection can be between 0 and 1\n",
    "    \n",
    "    return H\n",
    "\n",
    "\n",
    "def HCD(I, W_size, sigma, thresh):  # Harris Corner Detector algorithm\n",
    "    \n",
    "    assert W_size % 2 != 0  # Assert that window size is odd because the window must have a center\n",
    "\n",
    "    corners = [] # Corner coordinates\n",
    "    \n",
    "    # Create cornerness matrix\n",
    "    H = get_H(I, W_size, sigma)\n",
    "    \n",
    "    # Find the corners\n",
    "    for x in range(H.shape[0]):\n",
    "        for y in range(H.shape[1]):\n",
    "            \n",
    "            W_offset = int((W_size - 1) / 2)  # Offset from the center (x,y) to get the neighborhood using the given window size\n",
    "            \n",
    "            # Get neighborhood limits around center (x,y)\n",
    "            x_min = x - W_offset if x - W_offset >= 0 else 0\n",
    "            x_max = x + W_offset if x + W_offset <= H.shape[0] else H.shape[0]\n",
    "            y_min = y - W_offset if y - W_offset >= 0 else 0\n",
    "            y_max = y + W_offset if y + W_offset <= H.shape[1] else H.shape[1]\n",
    "            \n",
    "            N = H[x_min:x_max, y_min:y_max]  # Neighborhood around the center (x,y)\n",
    "            \n",
    "            if H[x,y] == np.max(N) and H[x,y] > thresh:  # If the point satisfies the corner conditions\n",
    "                corners.append([x,y])\n",
    "    \n",
    "    corners = np.array(corners)\n",
    "    return corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2\n",
    "\n",
    "def plot_HCD(I, I_x, I_y, corners):\n",
    "    \n",
    "    fig1, axs = plt.subplots(1, 2, figsize=(10,10))\n",
    "    \n",
    "    axs[0].set_title('Partial derivative Ix of I')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].imshow(I_x, cmap='gray')\n",
    "    \n",
    "    axs[1].set_title('Partial derivative Iy of I')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].imshow(I_y, cmap='gray')\n",
    "    \n",
    "    fig2 = plt.figure()\n",
    "    plt.title('Corners of image using HCD')\n",
    "    if corners.shape[0] != 0:  # If there are any corners\n",
    "        plt.scatter(corners[:,1], corners[:,0], c='red', s=10)\n",
    "    plt.imshow(I)\n",
    "\n",
    "    \n",
    "def run_HCD(img_dir, W_size, sigma, thresh, deg=0):\n",
    "    \n",
    "    print('Reading data...')\n",
    "    I = plt.imread(img_dir)\n",
    "    I = rotate(I, deg) if deg != 0 else I  # Rotate image if needed\n",
    "    I_gray = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)  # Read image as grayscale\n",
    "\n",
    "    print('Getting partial derivatives...')\n",
    "    I_x = get_I_x(I_gray)\n",
    "    I_y = get_I_y(I_gray)\n",
    "\n",
    "    print('Calculating corners...')\n",
    "    corners = HCD(I_gray, W_size, sigma, thresh)\n",
    "\n",
    "    print(f'Number of corners found: {corners.shape[0]}')\n",
    "\n",
    "    print('Plotting...\\n')\n",
    "    plot_HCD(I, I_x, I_y, corners)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2 (continue)\n",
    "\n",
    "# Toy\n",
    "run_HCD(img_dir='./data/toy/00000001.jpg', W_size=3, sigma=0.6, thresh=0.18)\n",
    "\n",
    "# Basketball\n",
    "run_HCD(img_dir='./data/basketball/085.jpg', W_size=3, sigma=0.5, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.3\n",
    "\n",
    "# Toy rotated by 45 degrees\n",
    "run_HCD(img_dir='./data/toy/00000001.jpg', W_size=3, sigma=0.6, thresh=0.23, deg=45)\n",
    "\n",
    "# Toy rotated by 90 degrees\n",
    "run_HCD(img_dir='./data/toy/00000001.jpg', W_size=3, sigma=0.6, thresh=0.18, deg=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green > Question - 2 (10-*pts*)**\n",
    "    \n",
    "Now you have seen the cornerness definition of Harris on $Equation ~(12)$. Another relevant definition of cornerness is defined by  *[Shi and Tomasi](https://ieeexplore.ieee.org/document/323794)*, after the original definition of Harris. Check their algorithm and answer the following questions:\n",
    "    \n",
    "1. How do they define cornerness? Write down their definition using the notations of $Equation ~(12)$.\n",
    "    \n",
    "\n",
    "2. Does the Shi-Tomasi Corner Detector satisfy the following properties: translation invariance, rotation invariance, scale invariance? Explain your reasoning.\n",
    "    \n",
    "\n",
    "3. In the following scenarios, what could be the relative cornerness values assigned by Shi and Tomasi? Explain your reasoning.\n",
    "    \n",
    "    (a) Both eigenvalues are near 0.\n",
    "    \n",
    "    (b) One eigenvalue is big and the other is near zero.\n",
    "    \n",
    "    (c) Both eigenvalues are big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q2*\n",
    "\n",
    "<br> \n",
    "\n",
    "1.\n",
    "According to J. Shi and C. Tomasi, detecting corners as features for motion tracking is not optimal as this method makes an assumption about the best window without taking into account what works best for the tracking algorithm. In their paper they use feature windows with a good score of texturedness (that is, it can be tracked well) which are selected by optimizing the tracker's accuracy to them. To select the windows, the eigenvalues of Z are calculated, and if both eigenvalues have big values, big enough to be over the user-defined threshold, then the window is selected as a feature. Two big eigenvalues represent patterns with a lot of texture information such as corners, salt-and-pepper textures and other patterns that can be tracked reliably.\n",
    "\n",
    "The equivalent \"cornerness\" score, or more accurately the \"texturedness\", is calculated as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(x,y) &=& min(\\lambda_1, \\lambda_2)\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "where Î»1, Î»2 are the eigenvalues of matrix Z for the window around center point (x,y):\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Z &=&\n",
    "\\begin{bmatrix}\n",
    "g_x^{2} & g_{x}g_{y}\\\\\n",
    "g_{x}g_{y}\\ & g_y^{2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The final selected feature windows are those where H(x,y) > Î».\n",
    "\n",
    "<br> \n",
    "\n",
    "2.\n",
    "- It is invariant to translation because the eigenvalues (that is, the cornerness) are extracted from matrix Z which contains the products of the partial derivatives and, as we know, derivation is invariant to shifting [1].\n",
    "- It is invariant to rotation as the cornerness matrix depends solely on the eigenvalues which, as we mentioned in Q1.3, are rotation invariant. \n",
    "- It is not invariant to scaling because, just like in HCD [1], when scaling the image without changing the window size, windows that contained corners will now appear as if they contain edges. Thus, the algorithm will not detect them as points of interest because the eigenvalues of the windows will be different, and thus the cornerness too.\n",
    "\n",
    "[1] https://www.cs.cornell.edu/courses/cs4670/2018sp/lec13-feature-sim.pdf.\n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "3.\n",
    "\n",
    "a. If both eigenvalues are near 0, the cornerness is 0 according to the above formula. That is, the intensity inside the window is almost constant [J. Shi, C. Tomasi] and, therefore, it doesn't contain any texture information as it is most likely a solid color and definitely not a corner, hence why the cornerness / texturedness is zero.\n",
    "    \n",
    "b. If only one of the eigenvalues is big and the other is close to 0, that means the pattern inside the window is unidirectional [J. Shi, C. Tomasi]. In other words, the texture contains lines or edges. Therefore, the cornerness according to the formula will be close to zero.\n",
    "\n",
    "c. If both eigenvalues have big values, then the cornerness will be equal to the smallest one of the two according to the formula. That is expected as in this case the window contains patterns with a lot of texture information such as corners [J. Shi, C. Tomasi].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optical Flow - Lucas-Kanade Algorithm (35pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optical flow is the apparent motion of image pixels or regions from one frame to the next, which results from moving objects in the image or from camera motion. Underlying optical flow is typically an assumption of $\\textit{brightness constancy}$. That is the image values (brightness, color, etc) remain constant over time, though their 2D position in the image may change. Algorithms for estimating optical flow exploit this assumption in various ways to compute a velocity field that describes the horizontal and vertical motion of every pixel in the image. For a 2D+t dimensional case a voxel at location $(x, y, t)$ with intensity $I(x, y, t)$ will have moved by $\\delta_x$, $\\delta_y$ and $\\delta_t$ between the two image frames, and the following image constraint equation can be given:\n",
    "\n",
    "\\begin{equation}\n",
    "I(x,y,t) = I(x+\\delta_x, y+\\delta_y, t+\\delta_t). \\tag{15}\n",
    "\\end{equation}\n",
    "\n",
    "Assuming the movement to be small, the image constraint at I(x, y, t) can be extended using Taylor series, truncated to first-order terms:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{16}\n",
    "I(x+\\delta_x, y+\\delta_y, t+\\delta_t) = I(x,y,t)+\\frac{\\partial I}{\\partial x}\\delta_x +\\frac{\\partial I}{\\partial y}\\delta_y+\\frac{\\partial I}{\\partial t}\\delta_t\n",
    "\\end{equation}\n",
    "\n",
    "Since we assume changes in the image can purely be attributed to movement, we will get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{17}\n",
    "\\frac{\\partial I}{\\partial x}\\frac{\\delta_x}{\\delta_t} +   \\frac{\\partial I}{\\partial y}\\frac{\\delta_y}{\\delta_t} + \\frac{\\partial I}{\\partial t}\\frac{\\delta_t}{\\delta_t} = 0\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{18}\n",
    "I_x V_x + I_y V_y = -I_t,\n",
    "\\end{equation}\n",
    "\n",
    "where $V_x$ and $V_y$ are the $x$ and $y$ components of the velocity or optical flow of $I(x,y,t)$. Further, $I_x$, $I_y$ and $I_t$ are the derivatives of the image at $(x, y, t)$ in the corresponding directions, which defines the main equation of optical flow.\n",
    "\n",
    "Optical flow is difficult to compute for two main reasons. First, in image regions that are roughly homogeneous, the optical flow is ambiguous, because the brightness constancy assumption is satisfied by many different motions. Second, in real scenes, the assumption is violated at motion boundaries and by miscellaneous lighting, non-rigid motions, shadows, transparency, reflections, etc. To address the former, all optical flow methods make some sort of assumption about the spatial variation of the optical flow that is used to resolve the ambiguity. Those are just assumptions about the world which are approximate and consequently may lead to errors in the flow estimates. The latter problem can be addressed by making much richer but more complicated assumptions about the changing image brightness or, more commonly, using robust statistical methods which can deal with 'violations' of the brightness constancy assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lucas-Kanade Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing the [Lucas-Kanade method](https://www.ri.cmu.edu/pub_files/pub3/lucas_bruce_d_1981_2/lucas_bruce_d_1981_2.pdf) for Optical Flow estimation. This method assumes that the optical flow is essentially constant in a local neighborhood of the pixel under consideration. Therefore, the main equation of the optical flow can be assumed to hold for all pixels within a window centered at the pixel under consideration. Let's consider pixel $p$. Then, for all pixels around $p$, the local image flow vector $(V_x, V_y)$ must satisfy:\n",
    "\n",
    "\\begin{array}{c}\n",
    "\\tag{19}\n",
    "  I_x(q_1)V_x + I_y(q_1)V_y = - I_t(q_1)\\\\\n",
    "  I_x(q_2)V_x + I_y(q_2)V_y = - I_t(q_2)\\\\\n",
    "  \\vdots \\\\\n",
    "  I_x(q_n)V_x + I_y(q_n)V_y = - I_t(q_n),\\\\\n",
    "\\end{array}\n",
    "\n",
    "where $q_1$, $q_2$, ... $q_n$ are the pixels inside the window around $p$. $I_x(q_i)$, $I_y(q_i)$, $I_t(q_i)$ are the partial derivatives of the image $I$ with respect to position $x$, $y$ and time $t$, evaluated at the point $q_i$ and at the current time. \n",
    "\n",
    "These equations can be written in matrix to form $Av=b$, where\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{20}\n",
    "A = \\begin{bmatrix}\n",
    "I_x(q_1) & I_y(q_1)\\\\\n",
    "I_x(q_2) & I_y(q_2)\\\\\n",
    "\\vdots   & \\vdots  \\\\\n",
    "I_x(q_n) & I_y(q_n)\n",
    "\\end{bmatrix},\n",
    "v = \\begin{bmatrix}\n",
    "V_x\\\\ V_y\n",
    "\\end{bmatrix}, \\mbox{ and }\n",
    "b = \\begin{bmatrix}\n",
    "-I_t(q_1)\\\\\n",
    "-I_t(q_2)\\\\\n",
    "\\vdots   \\\\\n",
    "-I_t(q_n)\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "This system has more equations than unknowns and thus it is usually over-determined. The Lucas-Kanade method obtains a compromise solution by the weighted-least-squares principle. Namely, it solves the $2\\times 2$ system as\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{21}\n",
    "A^T A v = A^T b\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "v = (A^T A)^{-1} A^T b.\n",
    "\\label{eqA} \\tag{22}\n",
    "\\end{equation}\n",
    "\n",
    "**<font color=green > Question - 1 (30-*pts*)**\n",
    "    \n",
    "For this assignment, you will be given three pairs of images: **sphere1.ppm**, **sphere2.ppm**; **synth1.pgm**, **synth2.pgm**; and **monkaa1.png**, **monkaa2.png**. You should estimate the optical flow between these two pairs. That is, you will get optical flow for sphere images, and for synth images separately. Implement the Lucas-Kanade algorithm using the following steps.\n",
    "\n",
    "    \n",
    "1. Divide  input  images  on  non-overlapping  regions,  each  region  being  $15\\times 15$.\n",
    "    \n",
    "    \n",
    "2. For each region compute $A$, $A^T$ and $b$. Then, estimate optical flow as given in $Equation~\\eqref{eqA}$.\n",
    "    \n",
    "    \n",
    "3. When you have estimation for optical flow $(V_x, V_y)$ of each region, you should display the results. There is a **matplotlib** function <font color=green >quiver</font> which plots a set of two-dimensional vectors as arrows on the screen. Try to figure out how to use this to show your optical flow results.\n",
    "\n",
    "<em>Note</em>: You are allowed to use $\\texttt{scipy.signal.convolve2d}$ to perform convolution.\n",
    "Include a demo function to run your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue > Hint**: You can use regions that are $15\\times 15$ pixels that are non-overlapping. That is, if input images are $256\\times 256$, you should have an array of $17\\times 17$ optical flow vectors at the end of your procedure. As we consider $15\\times 15$ regions, your matrix $\\textbf{A}$ will have the following size $225\\times 2$, and the vector $\\mathbf{b}$ will be $225\\times 1$.\n",
    "\n",
    "**<font color=blue > Hint**: Carefully read the documentation of $\\textbf{matplotlib}'s$ function <font color=\"green\">$\\texttt{quiver}$</font>. By default, the angles of the arrows are 45 degrees counter-clockwise from the horizontal axis. This means your arrows might point in the wrong direction! Also, play around with the arrow scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere1 = cv2.imread('data/sphere1.ppm')\n",
    "sphere2 = cv2.imread('data/sphere2.ppm')\n",
    "\n",
    "synth1 = cv2.imread('data/synth1.pgm')\n",
    "synth2 = cv2.imread('data/synth2.pgm')\n",
    "\n",
    "monkaa1 = cv2.imread('data/monkaa1.png')\n",
    "monkaa2 = cv2.imread('data/monkaa2.png')\n",
    "\n",
    "def Image_Regions(Img):\n",
    "    # Discard the would-be remaining pixels after dividing in 15x15 blocks.\n",
    "    Img_width = Img.shape[0] - (Img.shape[0] % 15)\n",
    "    Img_length = Img.shape[1] - (Img.shape[1] % 15)\n",
    "    regions = []\n",
    "    arrow_locs = []\n",
    "    \n",
    "    # Divide input images on non-overlapping regions, each region being  15Ã—15.\n",
    "    for j in range(0, Img_length, 15):\n",
    "        for i in range(0, Img_width, 15):\n",
    "            regions.append(Img[i:i+15, j:j+15])\n",
    "            # save center of region for plotting\n",
    "            arrow_locs.append([i+7, j+7])\n",
    "    return regions, arrow_locs\n",
    "\n",
    "def Optical_Flow(Rx, Ry, Rt):\n",
    "    A = np.vstack((Rx.flatten(), Ry.flatten())).T\n",
    "    # to prevent a singular matrix some trivial noise is added to A\n",
    "    A[0,0] += 0.0001\n",
    "    b = Rt.flatten()\n",
    "    v = np.linalg.inv(A.T @ A) @ A.T @ -b\n",
    "    return v\n",
    "            \n",
    "    \n",
    "def Plot_Lucas_Kanade(Img1, xlocs, ylocs, Vx, Vy):\n",
    "    plt.quiver(ylocs, xlocs, Vx, Vy, color = 'red', angles = 'xy', scale_units='xy')\n",
    "    plt.imshow(Img1, cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "def Lucas_Kanade(Img1, Img2):\n",
    "    Img1 = cv2.cvtColor(Img1, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    Img2 = cv2.cvtColor(Img2, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    \n",
    "    Ix = conv(Img1, np.array([[-1,0,1]]), mode='same') \n",
    "    Iy = conv(Img1, np.array([[-1,0,1]]).T, mode='same')\n",
    "    It = Img1 - Img2\n",
    "    \n",
    "    Rx, arrow_locs = Image_Regions(Ix)\n",
    "    Ry  = Image_Regions(Iy)[0]\n",
    "    Rt = Image_Regions(It)[0]\n",
    "    \n",
    "    xlocs = np.array(arrow_locs)[:, 0]\n",
    "    ylocs = np.array(arrow_locs)[:, 1]\n",
    "\n",
    "    V = []\n",
    "    for i in range(len(Rx)):\n",
    "        V.append(Optical_Flow(Rx[i], Ry[i], Rt[i]))\n",
    "    Vx = np.array(V)[:, 0]\n",
    "    Vy = np.array(V)[:, 1]\n",
    "    \n",
    "    Plot_Lucas_Kanade(Img1, xlocs, ylocs, Vx, Vy)\n",
    "    return xlocs, ylocs, Vx, Vy\n",
    "\n",
    "xlocs, ylocs, Vx, Vy = Lucas_Kanade(synth1, synth2)\n",
    "xlocs, ylocs, Vx, Vy = Lucas_Kanade(sphere1, sphere2)\n",
    "xlocs, ylocs, Vx, Vy = Lucas_Kanade(monkaa1, monkaa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green > Question - 2 (5-*pts*)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen one of the optical flow estimation methods developed by Lucas and Kanade. There are several more methods in the literature. The *[Horn-Schunck method](https://www.sciencedirect.com/science/article/abs/pii/0004370281900242)* is one of them. Check their method, compare it to Lucas-Kanade and answer the following questions:\n",
    "1. At what scale do the algorithms operate; i.e local or global? Explain your answer.\n",
    "\n",
    "\n",
    "2. How do the algorithms behave at flat regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Lucas-Kanade algorithm operates locally as it needs only a small subset of an image to determine optic flow, while the Horn-Schunck algorithm operates globally as it uses the entire image to determine optic flow.\n",
    "2. Because the Lucas-Kanade works locally, it will fail to capture movement on flat regions due to the aperture problem. The Horn-Schunck algorithm assumes the flow of an image varies smoothly almost everywhere in the image. Under this constraint, the algorithm is succeptible to distortions but is able to capture movement on flat regions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Tracking (20-pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will implement a simple feature-tracking algorithm. The aim is to extract visual features, like corners, and track them over multiple frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green > Question - 1 (18-*pts*)**\n",
    "\n",
    "1. Implement a simple feature-tracking algorithm by following below steps.\n",
    "    \n",
    "    (a) Locate feature points on the first frame by using the Harris Corner Detector, that you implemented in Section 1. \n",
    "    \n",
    "    (b) Track these points using the Lucas-Kanade algorithm for optical flow estimation, that you implemented in the Section 2.\n",
    "    \n",
    "    \n",
    "2. Prepare a video for each sample image sequences. These videos should visualize the initial feature points and the optical flow. Test your implementation and prepare visualization videos for **basketball** and **toy** samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include a demo function to run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - Type your code here\n",
    "f_basketball = './data/basketball/'\n",
    "f_toy = './data/toy/'\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "def make_video(folder, output_file, fourcc_str='mp4v', ):\n",
    "    ims = load_images_from_folder(folder)\n",
    "    h, w, layers = ims[0].shape\n",
    "    size = (w, h)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*fourcc_str)\n",
    "    output_vid = cv2.VideoWriter(output_file, fourcc, 15, size)\n",
    "    \n",
    "    for im in ims:\n",
    "        output_vid.write(im)\n",
    "    \n",
    "make_video(f_basketball, 'basketball.mp4')\n",
    "make_video(f_toy, 'toy.mp4')\n",
    "\n",
    "def play_video(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "   \n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            if cv2.waitKey(int(1000/15)) == ord('q'):\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "play_video('toy.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q1 - Type your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green > Question - 2 (2-*pts*)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need feature tracking even though we can detect features for each and every frame? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q2 - Type your answers here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
